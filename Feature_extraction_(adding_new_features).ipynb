{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import copy\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "from nltk.tree import Tree\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from hatesonar import Sonar\n",
    "from pprint import pprint\n",
    "from urlextract import URLExtract\n",
    "import syllables\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses\n",
    "tweetsAdr = \"./Serialization/Tweets/\"\n",
    "featureSerializationAdr = \"./Serialization/Features/\"\n",
    "tweetSerializationAdr = \"./Serialization/Tweets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries Setup\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = StanfordCoreNLP('')\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "sonar = Sonar()\n",
    "extractor = URLExtract()\n",
    "sentenceTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credible sources - USA\n",
    "\n",
    "adr1 = \"\"\n",
    "df = pd.read_csv(adr1, delimiter=\";\", skiprows=1 , names = [1,2,3,4,5,6,7,8,9], encoding = \"ISO-8859-1\")\n",
    "df = df.drop([1,5,6,7,8,9], axis=\"columns\")\n",
    "df = df.rename({2:\"source\", 3:\"id\", 4:\"website\"}, axis=\"columns\")\n",
    "\n",
    "df1 = df.drop(columns=[\"id\"], axis=\"columns\")\n",
    "df2 = df.drop(columns=[\"website\"], axis=\"columns\")\n",
    "\n",
    "df2[\"id\"] = df2[\"id\"].str.lower().str.replace(\"?\", \"\").str.strip()\n",
    "\n",
    "credibleWebsitesUsa = [i for i in df1.to_dict(orient=\"list\")[\"website\"] if str(i) != \"nan\"]\n",
    "credibleAccountsUsa = [j for j in df2.to_dict(orient=\"list\")[\"id\"] if str(j) != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credible sources - INDIA\n",
    "\n",
    "adr2 = \"\"\n",
    "df = pd.read_csv(adr2, delimiter=\";\", skiprows=1 , names=[1,2,3])\n",
    "df1 = df.drop(columns=[1,2])\n",
    "df2 = df.drop(columns=[1,3])\n",
    "df1 = df1.rename({2:\"twitter\", 3:\"site\"}, axis=\"columns\")\n",
    "df2 = df2.rename({2:\"twitter\", 3:\"site\"}, axis=\"columns\")\n",
    "\n",
    "df2[\"twitter\"] = df2[\"twitter\"].str.lower().str.strip()\n",
    "# df2[\"twitter\"] = df2[\"twitter\"].apply(lambda x: \"@\"+x)\n",
    "\n",
    "credibleWebsitesIndia = [q for q in df1.to_dict(orient=\"list\")[\"site\"] if str(q) != \"nan\"]\n",
    "credibleAccountsIndia = [p for p in df2.to_dict(orient=\"list\")[\"twitter\"] if str(p) != \"nan\"]\n",
    "\n",
    "credibleAccountsIndia = credibleAccountsIndia + credibleAccountsUsa\n",
    "credibleWebsitesIndia = credibleWebsitesIndia + credibleWebsitesUsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notorious sources\n",
    "\n",
    "notoriousSources = \"\"\n",
    "dfn = pd.read_csv(notoriousSources, delimiter=\";\")\n",
    "dfn = dfn.rename({\"test\":\"title\", \"Unnamed: 1\":\"id\", \"Unnamed: 2\":\"website\"}, axis=\"columns\")\n",
    "notoriousWebsites = [i.lower().strip() for i in dfn.to_dict(orient=\"list\")[\"website\"] if str(i) != \"nan\"]\n",
    "notoriousId = [i.lower().strip() for i in dfn.to_dict(orient=\"list\")[\"id\"] if str(i) != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading abbreviations, vuglar terms and emoticons for feature extraction\n",
    "\n",
    "abbrAdr = \"\"\n",
    "abbrList = [w.strip() for w in open(abbrAdr).readlines() if w != \"\\n\"]\n",
    "    \n",
    "emotiAdr = \"./Lists/Emoticon/emoticons.txt\"\n",
    "emotiList = [w.strip() for w in open(emotiAdr).readlines() if w != \"\\n\"]\n",
    "\n",
    "vuglarAdr = \"./Lists/Vuglar terms/vuglarTerms.txt\"\n",
    "vuglarList = [w.strip() for w in open(vuglarAdr).readlines() if w != \"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NRC dictionary for different feelings\n",
    "\n",
    "adr = \"\"\n",
    "\n",
    "nrcRaw = open(adr).readlines()\n",
    "nrcDic = {}\n",
    "for i in nrcRaw:\n",
    "    tmp = i.strip().split(\"\\t\")\n",
    "    lemma = tmp[0]\n",
    "    sentiment = tmp[1]\n",
    "    score = tmp[2]\n",
    "    if lemma in nrcDic.keys():\n",
    "        nrcDic[lemma][sentiment] = int(score)\n",
    "    else:\n",
    "        nrcDic[lemma] = {sentiment : int(score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Emotion dictionary\n",
    "\n",
    "adr = \"\"\n",
    "\n",
    "emotions = pd.DataFrame.from_csv(adr)\n",
    "emotions = emotions[[\"Word\", \"V.Mean.Sum\",\"A.Mean.Sum\",\"D.Mean.Sum\"]]\n",
    "emotions.columns = [\"word\",\"valence\",\"arousal\",\"dominance\"]\n",
    "emotions = emotions.T\n",
    "emotions.columns = emotions.loc[\"word\"]\n",
    "emotions = emotions.drop([\"word\"], axis=\"index\")\n",
    "emotionDic = pd.DataFrame.to_dict(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetElements(tweet):\n",
    "    rtFlag = True if \"retweeted_status\" in tweet.keys() else False\n",
    "    qtFlag = True if \"quoted_status\" in tweet.keys() else False \n",
    "\n",
    "    if rtFlag == False and qtFlag == False:\n",
    "        if tweet[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(tweet[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(tweet[\"text\"])\n",
    "            entities = copy.deepcopy(tweet[\"entities\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user]\n",
    "    \n",
    "    elif rtFlag == True and qtFlag == False:\n",
    "        rt = copy.deepcopy(tweet[\"retweeted_status\"])\n",
    "        if rt[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(rt[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(rt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(rt[\"text\"])\n",
    "            entities = copy.deepcopy(rt[\"entities\"])\n",
    "        rtUser = copy.deepcopy(rt[\"user\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user, rtUser]\n",
    "    \n",
    "    elif rtFlag == False and qtFlag == True:\n",
    "        if tweet[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(tweet[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(tweet[\"text\"])\n",
    "            entities = copy.deepcopy(tweet[\"entities\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        qt = copy.deepcopy(tweet[\"quoted_status\"])\n",
    "        if qt[\"truncated\"] == True:\n",
    "            qtText = copy.deepcopy(qt[\"extended_tweet\"][\"full_text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            qtText = copy.deepcopy(qt[\"text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"entities\"])\n",
    "        qtUser = copy.deepcopy(qt[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user, qtText, qtEntities, qtUser]        \n",
    "        \n",
    "    elif rtFlag == True and qtFlag == True:\n",
    "        rt = copy.deepcopy(tweet[\"retweeted_status\"])\n",
    "        qt = copy.deepcopy(tweet[\"quoted_status\"])        \n",
    "        if rt[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(rt[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(rt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(rt[\"text\"])\n",
    "            entities = copy.deepcopy(rt[\"entities\"])\n",
    "        rtUser = copy.deepcopy(rt[\"user\"])        \n",
    "        if qt[\"truncated\"] == True:\n",
    "            qtText = copy.deepcopy(qt[\"extended_tweet\"][\"full_text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            qtText = copy.deepcopy(qt[\"text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"entities\"])\n",
    "        qtUser = copy.deepcopy(qt[\"user\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user, qtText, qtEntities, qtUser, rtUser]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessor(twtTxt):\n",
    "    urls = extractor.find_urls(twtTxt)\n",
    "    semiRaw = twtTxt\n",
    "    for url in urls:\n",
    "        semiRaw = semiRaw.replace(url,\"\")\n",
    " \n",
    "    semiRaw = semiRaw.replace(\"  \", \" \").replace(\"  \", \" \").replace(\"\\t\", \" \").replace(\"\\n\", \" \").strip()\n",
    "    processed = semiRaw.replace(\"#\",\" \").replace(\"@\", \" \").replace(\"  \", \" \").replace(\"  \", \" \").replace(\"\\t\", \" \").replace(\"\\n\", \" \").strip()\n",
    "    overProcessed = processed.lower()\n",
    "    \n",
    "    return twtTxt, semiRaw, processed, overProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstPersonPronounCount(myTxt):\n",
    "    # Because in tokenization of I've, i'd, and I'm \"you\" will be separated, we do not ned to take care of such cases\n",
    "    comb_sing = word_tokenize(myTxt).count(\"i\") \\\n",
    "        + (word_tokenize(myTxt).count(\"my\") + word_tokenize(myTxt).count(\"mine\") + word_tokenize(myTxt).count(\"me\"))\n",
    "    \n",
    "    comb_plur = word_tokenize(myTxt).count(\"we\")  \\\n",
    "        + (word_tokenize(myTxt).count(\"our\") + word_tokenize(myTxt).count(\"ours\") + word_tokenize(myTxt).count(\"us\"))\n",
    " \n",
    "    return comb_sing + comb_plur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondPersonPronounCount(myText):\n",
    "    # Because in tokenization of you've, you'd, and you're \"you\" will be separated, we do not ned to take care of such cases\n",
    "    return word_tokenize(myText).count(\"you\")  \\\n",
    "            + word_tokenize(myText).count(\"your\") + word_tokenize(myText).count(\"yours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirdPersonPronounCount(myTweetTxt):\n",
    "    # The same comments as above for he, she, it, and they\n",
    "    sing = word_tokenize(myTweetTxt).count(\"he\") + \\\n",
    "            word_tokenize(myTweetTxt).count(\"she\")  + \\\n",
    "            word_tokenize(myTweetTxt).count(\"it\")  + \\\n",
    "            (word_tokenize(myTweetTxt).count(\"his\") + word_tokenize(myTweetTxt).count(\"her\") + \\\n",
    "             word_tokenize(myTweetTxt).count(\"its\") + word_tokenize(myTweetTxt).count(\"him\") + \\\n",
    "             word_tokenize(myTweetTxt).count(\"him\") +  word_tokenize(myTweetTxt).count(\"hers\"))\n",
    "    \n",
    "    comb = word_tokenize(myTweetTxt).count(\"they\")  + \\\n",
    "            (word_tokenize(myTweetTxt).count(\"their\") + word_tokenize(myTweetTxt).count(\"theirs\") + word_tokenize(myTweetTxt).count(\"them\"))\n",
    "     \n",
    "    return sing + comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrcEmotions(nrcTxt):\n",
    "    nrcTxtList = word_tokenize(nrcTxt)\n",
    "    \n",
    "    angerScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            angerScore += nrcDic[term][\"anger\"]\n",
    "    anticipationScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            anticipationScore += nrcDic[term][\"anticipation\"]\n",
    "    disgustScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            disgustScore += nrcDic[term][\"disgust\"]\n",
    "    fearScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            fearScore += nrcDic[term][\"fear\"]\n",
    "    joyScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            joyScore += nrcDic[term][\"joy\"]\n",
    "    sadnessScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            sadnessScore += nrcDic[term][\"sadness\"]\n",
    "    surpriseScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            surpriseScore += nrcDic[term][\"surprise\"]\n",
    "    trustScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            trustScore += nrcDic[term][\"trust\"]\n",
    "    \n",
    "    return angerScore, anticipationScore, disgustScore, fearScore, joyScore, sadnessScore, surpriseScore, trustScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotions(semiRaw, overProc):\n",
    "    posScore = analyser.polarity_scores(semiRaw)[\"pos\"]\n",
    "    negScore = analyser.polarity_scores(semiRaw)[\"neg\"]\n",
    "    neuScore = analyser.polarity_scores(semiRaw)[\"neu\"]\n",
    "    compScore = analyser.polarity_scores(semiRaw)[\"compound\"]\n",
    "\n",
    "    wordList = word_tokenize(overProc)\n",
    "    arousalScore = 0\n",
    "    for term in wordList:\n",
    "        if term in emotionDic:\n",
    "            arousalScore += emotionDic[term][\"arousal\"]\n",
    "\n",
    "    dominanceScore = 0\n",
    "    for term in wordList:\n",
    "        if term in emotionDic:\n",
    "            dominanceScore += emotionDic[term][\"dominance\"]\n",
    "            \n",
    "    return posScore, negScore, neuScore, compScore, arousalScore, dominanceScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hateSpeech(tweetTxt):\n",
    "    sonar2 = sonar.ping(tweetTxt)\n",
    "    return sonar2[\"classes\"][0][\"confidence\"], sonar2[\"classes\"][1][\"confidence\"], sonar2[\"classes\"][2][\"confidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleTruth(first, second, third):\n",
    "    if first == True and second == False and third == False:\n",
    "        return True\n",
    "    elif first == False and second == True and third == False:\n",
    "        return True\n",
    "    elif first == False and second == False and third == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleTruth(first, second, third):\n",
    "    if first == True and second == True and third == False:\n",
    "        return True\n",
    "    elif first == False and second == True and third == True:\n",
    "        return True\n",
    "    elif first == True and second == False and third == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keralaRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"keralaRumour\" in i]\n",
    "counter = 0\n",
    "keralaRumourId = {}\n",
    "keralaRumoursFeatures = {}\n",
    "\n",
    "for file in keralaRumourFolder:\n",
    "    keralaRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(keralaRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in keralaRumourId:\n",
    "            keralaRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            keralaRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ Rumour General Info ################################################################\n",
    "\n",
    "        keralaRumoursFeatures[tweetId]={}\n",
    "        keralaRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        keralaRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        keralaRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "\n",
    "        # This is temporary for some new features\n",
    "        nrcTxtList = word_tokenize(overProcessedText)\n",
    "        angerScore = 0\n",
    "        for term in nrcTxtList:\n",
    "            if term in nrcDic:\n",
    "                angerScore += nrcDic[term][\"anger\"]\n",
    "        keralaRumoursFeatures[tweetId][\"angerEmotion\"] = angerScore\n",
    "        \n",
    "        keralaRumoursFeatures[tweetId][\"flesch_reading_ease\"] = textstat.flesch_reading_ease(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"smog_index\"] = textstat.smog_index(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"coleman_liau_index\"] = textstat.coleman_liau_index(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"automated_readability_index\"] = textstat.automated_readability_index(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"difficult_words\"] = textstat.difficult_words(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"linsear_write_formula\"] = textstat.linsear_write_formula(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"gunning_fog\"] = textstat.gunning_fog(overProcessedText)\n",
    "        \n",
    "        \n",
    "        # One of the syllabus library pitfall is that, an empty sentence is one syllabus\n",
    "        # To avoid devision by zero error\n",
    "        try: \n",
    "            keralaRumoursFeatures[tweetId][\"averageWordsyllables\"] = np.average([syllables.estimate(i) for i in word_tokenize(overProcessedText)])\n",
    "        except:\n",
    "            keralaRumoursFeatures[tweetId][\"averageWordsyllables\"] = 0            \n",
    "        \n",
    "\n",
    "        ### evidence credibility  \n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "        \n",
    "        keralaRumoursFeatures[tweetId][\"urlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        keralaRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        \n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"quoteReputation\"] = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        keralaRumoursFeatures[tweetId][\"quoteNotoriety\"] = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "\n",
    "    pk.dump(keralaRumoursFeatures, open(f'{featureSerializationAdr}keralaRumoursFeatures_SomeNewFeatures_{counter}.pk', \"wb\")) # This is temporary for adding new features\n",
    "    keralaRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is temporary to add some new features\n",
    "keralaRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"keralaRumoursFeatures_SomeNewFeatures_\" in i]\n",
    "keralaRumoursFeaturesList = []\n",
    "for i in keralaRumoursFeaturesFolder:\n",
    "    keralaRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "keralaRumoursFeatures = pd.concat([df for df in keralaRumoursFeaturesList])\n",
    "keralaRumoursFeatures[\"id\"] = keralaRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(keralaRumoursFeatures, open(f'{featureSerializationAdr}/keralaRumoursFeaturesWithoutLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "keralaRumoursLIWC = pk.load(open(featureSerializationAdr+\"keralaRumoursLIWC.pk\", \"rb\"))\n",
    "keralaRumoursFeaturesWithLIWC = pd.merge(keralaRumoursFeatures, keralaRumoursLIWC, on=\"id\")\n",
    "pk.dump(keralaRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/keralaRumoursFeaturesWithLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldkeralaRumoursLIWC = pk.load(open(featureSerializationAdr+\"keralaRumoursFeaturesWithoutLIWC.pk\", \"rb\"))\n",
    "\n",
    "oldkeralaRumoursLIWC = oldkeralaRumoursLIWC.drop(columns=['averageSentenceComplexity', 'averageWordComplexity','sentenceCount'])\n",
    "\n",
    "keralaRumoursWithLIWC_NewFeatures = pd.merge(oldkeralaRumoursLIWC, keralaRumoursFeaturesWithLIWC, on=\"id\")\n",
    "\n",
    "pk.dump(keralaRumoursWithLIWC_NewFeatures, open(f'{featureSerializationAdr}/keralaRumoursWithLIWC_NewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keralaNonRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"keralaNonRumour\" in i]\n",
    "counter = 0\n",
    "keralaNonRumourId = {}\n",
    "keralaNonRumoursFeatures = {}\n",
    "\n",
    "for file in keralaNonRumourFolder:\n",
    "    keralaNonRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(keralaNonRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in keralaNonRumourId:\n",
    "            keralaNonRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            keralaNonRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ NonRumour General Info ################################################################\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId]={}\n",
    "        keralaNonRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "\n",
    "        # This is temporary for some new features\n",
    "        nrcTxtList = word_tokenize(overProcessedText)\n",
    "        angerScore = 0\n",
    "        for term in nrcTxtList:\n",
    "            if term in nrcDic:\n",
    "                angerScore += nrcDic[term][\"anger\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"angerEmotion\"] = angerScore\n",
    "        \n",
    "        keralaNonRumoursFeatures[tweetId][\"flesch_reading_ease\"] = textstat.flesch_reading_ease(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"smog_index\"] = textstat.smog_index(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"coleman_liau_index\"] = textstat.coleman_liau_index(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"automated_readability_index\"] = textstat.automated_readability_index(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"difficult_words\"] = textstat.difficult_words(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"linsear_write_formula\"] = textstat.linsear_write_formula(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"gunning_fog\"] = textstat.gunning_fog(overProcessedText)\n",
    "        \n",
    "        \n",
    "        # One of the syllabus library pitfall is that, an empty sentence is one syllabus\n",
    "        # To avoid devision by zero error\n",
    "        try: \n",
    "            keralaNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = np.average([syllables.estimate(i) for i in word_tokenize(overProcessedText)])\n",
    "        except:\n",
    "            keralaNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = 0      \n",
    "        ### evidence credibility\n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "        \n",
    "        keralaNonRumoursFeatures[tweetId][\"urlNotoriety\"]  = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"quoteReputation\"]  = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        keralaNonRumoursFeatures[tweetId][\"quoteNotoriety\"]  = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "\n",
    "    pk.dump(keralaNonRumoursFeatures, open(f'{featureSerializationAdr}keralaNonRumoursFeatures_SomeNewFeatures_{counter}.pk', \"wb\"))\n",
    "    keralaNonRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is temporary to add some new features\n",
    "keralaNonRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"keralaNonRumoursFeatures_SomeNewFeatures_\" in i]\n",
    "keralaNonRumoursFeaturesList = []\n",
    "for i in keralaNonRumoursFeaturesFolder:\n",
    "    keralaNonRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "keralaNonRumoursFeatures = pd.concat([df for df in keralaNonRumoursFeaturesList])\n",
    "keralaNonRumoursFeatures[\"id\"] = keralaNonRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(keralaNonRumoursFeatures, open(f'{featureSerializationAdr}/keralaNonRumoursFeaturesWithoutLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "keralaNonRumoursLIWC = pk.load(open(featureSerializationAdr+\"keralaNonRumoursLIWC.pk\", \"rb\"))\n",
    "keralaNonRumoursFeaturesWithLIWC = pd.merge(keralaNonRumoursFeatures, keralaNonRumoursLIWC, on=\"id\")\n",
    "pk.dump(keralaNonRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/keralaNonRumoursFeaturesWithLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldkeralaNonRumoursLIWC = pk.load(open(featureSerializationAdr+\"keralaNonRumoursFeaturesWithoutLIWC.pk\", \"rb\"))\n",
    "\n",
    "oldkeralaNonRumoursLIWC = oldkeralaNonRumoursLIWC.drop(columns=['averageSentenceComplexity', 'averageWordComplexity','sentenceCount'])\n",
    "\n",
    "keralaNonRumoursWithLIWC_NewFeatures = pd.merge(oldkeralaNonRumoursLIWC, keralaNonRumoursFeaturesWithLIWC, on=\"id\")\n",
    "\n",
    "pk.dump(keralaNonRumoursWithLIWC_NewFeatures, open(f'{featureSerializationAdr}/keralaNonRumoursWithLIWC_NewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florenceRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"florenceRumour\" in i]\n",
    "counter = 0\n",
    "florenceRumourId = {}\n",
    "florenceRumoursFeatures = {}\n",
    "\n",
    "for file in florenceRumourFolder:\n",
    "    florenceRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(florenceRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in florenceRumourId:\n",
    "            florenceRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            florenceRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ Rumour General Info ################################################################\n",
    "\n",
    "        florenceRumoursFeatures[tweetId]={}\n",
    "        florenceRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        florenceRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        florenceRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "\n",
    "        ### evidence credibility    \n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "        \n",
    "        florenceRumoursFeatures[tweetId][\"urlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        florenceRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"quoteReputation\"] = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        florenceRumoursFeatures[tweetId][\"quoteNotoriety\"] = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "    pk.dump(florenceRumoursFeatures, open(f'{featureSerializationAdr}florenceRumoursFeatures_SomeNewFeatures_{counter}.pk', \"wb\"))\n",
    "    florenceRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the features as one pandas dataframe\n",
    "florenceRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"florenceRumoursFeatures_SomeNewFeatures_\" in i]\n",
    "florenceRumoursFeaturesList = []\n",
    "for i in florenceRumoursFeaturesFolder:\n",
    "    florenceRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "florenceRumoursFeatures = pd.concat([df for df in florenceRumoursFeaturesList])\n",
    "florenceRumoursFeatures[\"id\"] = florenceRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(florenceRumoursFeatures, open(f'{featureSerializationAdr}/florenceRumoursFeaturesWithoutLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "florenceRumoursLIWC = pk.load(open(featureSerializationAdr+\"florenceRumoursLIWC.pk\", \"rb\"))\n",
    "florenceRumoursFeaturesWithLIWC = pd.merge(florenceRumoursFeatures, florenceRumoursLIWC, on=\"id\")\n",
    "pk.dump(florenceRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/florenceRumoursFeaturesWithLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldkeralaRumoursLIWC = pk.load(open(featureSerializationAdr+\"keralaRumoursFeaturesWithoutLIWC.pk\", \"rb\"))\n",
    "\n",
    "oldkeralaRumoursLIWC = oldkeralaRumoursLIWC.drop(columns=['averageSentenceComplexity', 'averageWordComplexity','sentenceCount'])\n",
    "\n",
    "keralaRumoursWithLIWC_NewFeatures = pd.merge(oldkeralaRumoursLIWC, keralaRumoursFeaturesWithLIWC, on=\"id\")\n",
    "\n",
    "pk.dump(keralaRumoursWithLIWC_NewFeatures, open(f'{featureSerializationAdr}/keralaRumoursWithLIWC_NewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florenceNonRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"florenceNonRumour\" in i]\n",
    "counter = 0\n",
    "florenceNonRumourId = {}\n",
    "florenceNonRumoursFeatures = {}\n",
    "\n",
    "for file in florenceNonRumourFolder:\n",
    "    florenceNonRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(florenceNonRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in florenceNonRumourId:\n",
    "            florenceNonRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            florenceNonRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ NonRumour General Info ################################################################\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId]={}\n",
    "        florenceNonRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "        \n",
    "    # This is temporary for some new features\n",
    "        nrcTxtList = word_tokenize(overProcessedText)\n",
    "        angerScore = 0\n",
    "        for term in nrcTxtList:\n",
    "            if term in nrcDic:\n",
    "                angerScore += nrcDic[term][\"anger\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"angerEmotion\"] = angerScore\n",
    "        \n",
    "        florenceNonRumoursFeatures[tweetId][\"flesch_reading_ease\"] = textstat.flesch_reading_ease(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"smog_index\"] = textstat.smog_index(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"coleman_liau_index\"] = textstat.coleman_liau_index(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"automated_readability_index\"] = textstat.automated_readability_index(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"difficult_words\"] = textstat.difficult_words(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"linsear_write_formula\"] = textstat.linsear_write_formula(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"gunning_fog\"] = textstat.gunning_fog(overProcessedText)\n",
    "        \n",
    "        \n",
    "        # One of the syllabus library pitfall is that, an empty sentence is one syllabus\n",
    "        # To avoid devision by zero error\n",
    "        try: \n",
    "            florenceNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = np.average([syllables.estimate(i) for i in word_tokenize(overProcessedText)])\n",
    "        except:\n",
    "            florenceNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = 0   \n",
    "         ################################################################ NonRumourmonger Features ################################################################\n",
    "\n",
    "        ### evidence credibility \n",
    "\n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "        \n",
    "        florenceNonRumoursFeatures[tweetId][\"urlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"quoteReputation\"] = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        florenceNonRumoursFeatures[tweetId][\"quoteNotoriety\"] = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "\n",
    "    pk.dump(florenceNonRumoursFeatures, open(f'{featureSerializationAdr}florenceNonRumoursFeatures_SomeNewFeatures_{counter}.pk', \"wb\"))\n",
    "    florenceNonRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the features as one pandas dataframe\n",
    "florenceNonRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"florenceNonRumoursFeatures_SomeNewFeatures_\" in i]\n",
    "florenceNonRumoursFeaturesList = []\n",
    "for i in florenceNonRumoursFeaturesFolder:\n",
    "    florenceNonRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "florenceNonRumoursFeatures = pd.concat([df for df in florenceNonRumoursFeaturesList])\n",
    "florenceNonRumoursFeatures[\"id\"] = florenceNonRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(florenceNonRumoursFeatures, open(f'{featureSerializationAdr}/florenceNonRumoursFeaturesWithoutLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "florenceNonRumoursLIWC = pk.load(open(featureSerializationAdr+\"florenceNonRumoursLIWC.pk\", \"rb\"))\n",
    "florenceNonRumoursFeaturesWithLIWC = pd.merge(florenceNonRumoursFeatures, florenceNonRumoursLIWC, on=\"id\")\n",
    "pk.dump(florenceNonRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/florenceNonRumoursFeaturesWithLIWC_SomeNewFeatures.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldflorenceNonRumoursLIWC = pk.load(open(featureSerializationAdr+\"florenceNonRumoursFeaturesWithoutLIWC.pk\", \"rb\"))\n",
    "\n",
    "oldflorenceNonRumoursLIWC = oldflorenceNonRumoursLIWC.drop(columns=['averageSentenceComplexity', 'averageWordComplexity','sentenceCount'])\n",
    "\n",
    "florenceNonRumoursWithLIWC_NewFeatures = pd.merge(oldflorenceNonRumoursLIWC, florenceNonRumoursFeaturesWithLIWC, on=\"id\")\n",
    "\n",
    "pk.dump(florenceNonRumoursWithLIWC_NewFeatures, open(f'{featureSerializationAdr}/florenceNonRumoursWithLIWC_NewFeatures.pk', \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
