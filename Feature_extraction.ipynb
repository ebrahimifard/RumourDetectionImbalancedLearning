{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import copy\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "from nltk.tree import Tree\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from hatesonar import Sonar\n",
    "from pprint import pprint\n",
    "from urlextract import URLExtract\n",
    "import syllables\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model download\n",
    "# stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "# models_dir_adr = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses\n",
    "tweetsAdr = \"./Serialization/Tweets/\"\n",
    "featureSerializationAdr = \"./Serialization/Features/\"\n",
    "tweetSerializationAdr = \"./Serialization/Tweets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries Setup\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = StanfordCoreNLP('')\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "sonar = Sonar()\n",
    "extractor = URLExtract()\n",
    "sentenceTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credible sources - USA\n",
    "\n",
    "adr1 = \"\"\n",
    "df = pd.read_csv(adr1, delimiter=\";\", skiprows=1 , names = [1,2,3,4,5,6,7,8,9], encoding = \"ISO-8859-1\")\n",
    "df = df.drop([1,5,6,7,8,9], axis=\"columns\")\n",
    "df = df.rename({2:\"source\", 3:\"id\", 4:\"website\"}, axis=\"columns\")\n",
    "\n",
    "df1 = df.drop(columns=[\"id\"], axis=\"columns\")\n",
    "df2 = df.drop(columns=[\"website\"], axis=\"columns\")\n",
    "\n",
    "df2[\"id\"] = df2[\"id\"].str.lower().str.replace(\"?\", \"\").str.strip()\n",
    "\n",
    "credibleWebsitesUsa = [i for i in df1.to_dict(orient=\"list\")[\"website\"] if str(i) != \"nan\"]\n",
    "credibleAccountsUsa = [j for j in df2.to_dict(orient=\"list\")[\"id\"] if str(j) != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credible sources - INDIA\n",
    "\n",
    "adr2 = \"\"\n",
    "df = pd.read_csv(adr2, delimiter=\";\", skiprows=1 , names=[1,2,3])\n",
    "df1 = df.drop(columns=[1,2])\n",
    "df2 = df.drop(columns=[1,3])\n",
    "df1 = df1.rename({2:\"twitter\", 3:\"site\"}, axis=\"columns\")\n",
    "df2 = df2.rename({2:\"twitter\", 3:\"site\"}, axis=\"columns\")\n",
    "\n",
    "df2[\"twitter\"] = df2[\"twitter\"].str.lower().str.strip()\n",
    "\n",
    "credibleWebsitesIndia = [q for q in df1.to_dict(orient=\"list\")[\"site\"] if str(q) != \"nan\"]\n",
    "credibleAccountsIndia = [p for p in df2.to_dict(orient=\"list\")[\"twitter\"] if str(p) != \"nan\"]\n",
    "\n",
    "credibleAccountsIndia = credibleAccountsIndia + credibleAccountsUsa\n",
    "credibleWebsitesIndia = credibleWebsitesIndia + credibleWebsitesUsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notorious sources\n",
    "\n",
    "notoriousSources = \"\"\n",
    "dfn = pd.read_csv(notoriousSources, delimiter=\";\")\n",
    "dfn = dfn.rename({\"test\":\"title\", \"Unnamed: 1\":\"id\", \"Unnamed: 2\":\"website\"}, axis=\"columns\")\n",
    "notoriousWebsites = [i.lower().strip() for i in dfn.to_dict(orient=\"list\")[\"website\"] if str(i) != \"nan\"]\n",
    "notoriousId = [i.lower().strip() for i in dfn.to_dict(orient=\"list\")[\"id\"] if str(i) != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading abbreviations, vuglar terms and emoticons for feature extraction\n",
    "\n",
    "abbrAdr = \"\"\n",
    "abbrList = [w.strip() for w in open(abbrAdr).readlines() if w != \"\\n\"]\n",
    "    \n",
    "emotiAdr = \"\"\n",
    "emotiList = [w.strip() for w in open(emotiAdr).readlines() if w != \"\\n\"]\n",
    "\n",
    "vuglarAdr = \"\"\n",
    "vuglarList = [w.strip() for w in open(vuglarAdr).readlines() if w != \"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NRC dictionary for different feelings\n",
    "\n",
    "adr = \"\"\n",
    "\n",
    "nrcRaw = open(adr).readlines()\n",
    "nrcDic = {}\n",
    "for i in nrcRaw:\n",
    "    tmp = i.strip().split(\"\\t\")\n",
    "    lemma = tmp[0]\n",
    "    sentiment = tmp[1]\n",
    "    score = tmp[2]\n",
    "    if lemma in nrcDic.keys():\n",
    "        nrcDic[lemma][sentiment] = int(score)\n",
    "    else:\n",
    "        nrcDic[lemma] = {sentiment : int(score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Emotion dictionary\n",
    "\n",
    "adr = \"\"\n",
    "\n",
    "emotions = pd.DataFrame.from_csv(adr)\n",
    "emotions = emotions[[\"Word\", \"V.Mean.Sum\",\"A.Mean.Sum\",\"D.Mean.Sum\"]]\n",
    "emotions.columns = [\"word\",\"valence\",\"arousal\",\"dominance\"]\n",
    "emotions = emotions.T\n",
    "emotions.columns = emotions.loc[\"word\"]\n",
    "emotions = emotions.drop([\"word\"], axis=\"index\")\n",
    "emotionDic = pd.DataFrame.to_dict(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetElements(tweet):\n",
    "    rtFlag = True if \"retweeted_status\" in tweet.keys() else False\n",
    "    qtFlag = True if \"quoted_status\" in tweet.keys() else False \n",
    "\n",
    "    if rtFlag == False and qtFlag == False:\n",
    "        if tweet[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(tweet[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(tweet[\"text\"])\n",
    "            entities = copy.deepcopy(tweet[\"entities\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user]\n",
    "    \n",
    "    elif rtFlag == True and qtFlag == False:\n",
    "        rt = copy.deepcopy(tweet[\"retweeted_status\"])\n",
    "        if rt[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(rt[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(rt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(rt[\"text\"])\n",
    "            entities = copy.deepcopy(rt[\"entities\"])\n",
    "        rtUser = copy.deepcopy(rt[\"user\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user, rtUser]\n",
    "    \n",
    "    elif rtFlag == False and qtFlag == True:\n",
    "        if tweet[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(tweet[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(tweet[\"text\"])\n",
    "            entities = copy.deepcopy(tweet[\"entities\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        qt = copy.deepcopy(tweet[\"quoted_status\"])\n",
    "        if qt[\"truncated\"] == True:\n",
    "            qtText = copy.deepcopy(qt[\"extended_tweet\"][\"full_text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            qtText = copy.deepcopy(qt[\"text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"entities\"])\n",
    "        qtUser = copy.deepcopy(qt[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user, qtText, qtEntities, qtUser]        \n",
    "        \n",
    "    elif rtFlag == True and qtFlag == True:\n",
    "        rt = copy.deepcopy(tweet[\"retweeted_status\"])\n",
    "        qt = copy.deepcopy(tweet[\"quoted_status\"])        \n",
    "        if rt[\"truncated\"] == True:\n",
    "            text = copy.deepcopy(rt[\"extended_tweet\"][\"full_text\"])\n",
    "            entities = copy.deepcopy(rt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            text = copy.deepcopy(rt[\"text\"])\n",
    "            entities = copy.deepcopy(rt[\"entities\"])\n",
    "        rtUser = copy.deepcopy(rt[\"user\"])        \n",
    "        if qt[\"truncated\"] == True:\n",
    "            qtText = copy.deepcopy(qt[\"extended_tweet\"][\"full_text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"extended_tweet\"][\"entities\"])\n",
    "        else:\n",
    "            qtText = copy.deepcopy(qt[\"text\"])\n",
    "            qtEntities = copy.deepcopy(qt[\"entities\"])\n",
    "        qtUser = copy.deepcopy(qt[\"user\"])\n",
    "        user = copy.deepcopy(tweet[\"user\"])\n",
    "        return [rtFlag, qtFlag, text, entities, user, qtText, qtEntities, qtUser, rtUser]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALSO PROVIDE ONE VERSION WITHOUT PUNCTUATION\n",
    "### AND ONE VERSION THAT USER CAN DETERMINE WHICH PUNCTUATION SHE WANTS OR DOESN'T WANT \n",
    "def textProcessor(twtTxt):\n",
    "    urls = extractor.find_urls(twtTxt)\n",
    "    semiRaw = twtTxt\n",
    "    for url in urls:\n",
    "        semiRaw = semiRaw.replace(url,\"\")\n",
    " \n",
    "    semiRaw = semiRaw.replace(\"  \", \" \").replace(\"  \", \" \").replace(\"\\t\", \" \").replace(\"\\n\", \" \").strip()\n",
    "    processed = semiRaw.replace(\"#\",\" \").replace(\"@\", \" \").replace(\"  \", \" \").replace(\"  \", \" \").replace(\"\\t\", \" \").replace(\"\\n\", \" \").strip()\n",
    "    overProcessed = processed.lower()\n",
    "    \n",
    "    return twtTxt, semiRaw, processed, overProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstPersonPronounCount(myTxt):\n",
    "    # Because in tokenization of I've, i'd, and I'm \"you\" will be separated, we do not ned to take care of such cases\n",
    "    comb_sing = word_tokenize(myTxt).count(\"i\") \\\n",
    "        + (word_tokenize(myTxt).count(\"my\") + word_tokenize(myTxt).count(\"mine\") + word_tokenize(myTxt).count(\"me\"))\n",
    "    \n",
    "    comb_plur = word_tokenize(myTxt).count(\"we\")  \\\n",
    "        + (word_tokenize(myTxt).count(\"our\") + word_tokenize(myTxt).count(\"ours\") + word_tokenize(myTxt).count(\"us\"))\n",
    " \n",
    "    return comb_sing + comb_plur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondPersonPronounCount(myText):\n",
    "    # Because in tokenization of you've, you'd, and you're \"you\" will be separated, we do not ned to take care of such cases\n",
    "    return word_tokenize(myText).count(\"you\")  \\\n",
    "            + word_tokenize(myText).count(\"your\") + word_tokenize(myText).count(\"yours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirdPersonPronounCount(myTweetTxt):\n",
    "    # The same comments as above for he, she, it, and they\n",
    "    sing = word_tokenize(myTweetTxt).count(\"he\") + \\\n",
    "            word_tokenize(myTweetTxt).count(\"she\")  + \\\n",
    "            word_tokenize(myTweetTxt).count(\"it\")  + \\\n",
    "            (word_tokenize(myTweetTxt).count(\"his\") + word_tokenize(myTweetTxt).count(\"her\") + \\\n",
    "             word_tokenize(myTweetTxt).count(\"its\") + word_tokenize(myTweetTxt).count(\"him\") + \\\n",
    "             word_tokenize(myTweetTxt).count(\"him\") +  word_tokenize(myTweetTxt).count(\"hers\"))\n",
    "    \n",
    "    comb = word_tokenize(myTweetTxt).count(\"they\")  + \\\n",
    "            (word_tokenize(myTweetTxt).count(\"their\") + word_tokenize(myTweetTxt).count(\"theirs\") + word_tokenize(myTweetTxt).count(\"them\"))\n",
    "     \n",
    "    return sing + comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrcEmotions(nrcTxt):\n",
    "    nrcTxtList = word_tokenize(nrcTxt)\n",
    "    \n",
    "    angerScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            angerScore += nrcDic[term][\"anger\"]\n",
    "    anticipationScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            anticipationScore += nrcDic[term][\"anticipation\"]\n",
    "    disgustScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            disgustScore += nrcDic[term][\"disgust\"]\n",
    "    fearScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            fearScore += nrcDic[term][\"fear\"]\n",
    "    joyScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            joyScore += nrcDic[term][\"joy\"]\n",
    "    sadnessScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            sadnessScore += nrcDic[term][\"sadness\"]\n",
    "    surpriseScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            surpriseScore += nrcDic[term][\"surprise\"]\n",
    "    trustScore = 0\n",
    "    for term in nrcTxtList:\n",
    "        if term in nrcDic:\n",
    "            trustScore += nrcDic[term][\"trust\"]\n",
    "    \n",
    "    return angerScore, anticipationScore, disgustScore, fearScore, joyScore, sadnessScore, surpriseScore, trustScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotions(semiRaw, overProc):\n",
    "    posScore = analyser.polarity_scores(semiRaw)[\"pos\"]\n",
    "    negScore = analyser.polarity_scores(semiRaw)[\"neg\"]\n",
    "    neuScore = analyser.polarity_scores(semiRaw)[\"neu\"]\n",
    "    compScore = analyser.polarity_scores(semiRaw)[\"compound\"]\n",
    "\n",
    "    wordList = word_tokenize(overProc)\n",
    "    arousalScore = 0\n",
    "    for term in wordList:\n",
    "        if term in emotionDic:\n",
    "            arousalScore += emotionDic[term][\"arousal\"]\n",
    "\n",
    "    dominanceScore = 0\n",
    "    for term in wordList:\n",
    "        if term in emotionDic:\n",
    "            dominanceScore += emotionDic[term][\"dominance\"]\n",
    "            \n",
    "    return posScore, negScore, neuScore, compScore, arousalScore, dominanceScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hateSpeech(tweetTxt):\n",
    "    sonar2 = sonar.ping(tweetTxt)\n",
    "    return sonar2[\"classes\"][0][\"confidence\"], sonar2[\"classes\"][1][\"confidence\"], sonar2[\"classes\"][2][\"confidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleTruth(first, second, third):\n",
    "    if first == True and second == False and third == False:\n",
    "        return True\n",
    "    elif first == False and second == True and third == False:\n",
    "        return True\n",
    "    elif first == False and second == False and third == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleTruth(first, second, third):\n",
    "    if first == True and second == True and third == False:\n",
    "        return True\n",
    "    elif first == False and second == True and third == True:\n",
    "        return True\n",
    "    elif first == True and second == False and third == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keralaRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"keralaRumour\" in i]\n",
    "counter = 0\n",
    "keralaRumourId = {}\n",
    "keralaRumoursFeatures = {}\n",
    "\n",
    "for file in keralaRumourFolder:\n",
    "    keralaRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(keralaRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in keralaRumourId:\n",
    "            keralaRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            keralaRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ Rumour General Info ################################################################\n",
    "\n",
    "        keralaRumoursFeatures[tweetId]={}\n",
    "        keralaRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        keralaRumoursFeatures[tweetId][\"screenName\"]=user[\"screen_name\"]\n",
    "        keralaRumoursFeatures[tweetId][\"text\"]=text\n",
    "        keralaRumoursFeatures[tweetId][\"tweetUrl\"] = \"https://twitter.com/\" + user[\"screen_name\"] + \"/status/\" + str(tweetId)\n",
    "\n",
    "    ##### =>\n",
    "        pattern = re.compile('[>].*[<]')\n",
    "        try:\n",
    "            keralaRumoursFeatures[tweetId][\"source\"] = pattern.findall(tweet[\"source\"])[0][1:-1]\n",
    "        except:\n",
    "            keralaRumoursFeatures[tweetId][\"source\"] = tweet[\"source\"]\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"tweetPostTime\"] = datetime.datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    #### =>\n",
    "        keralaRumoursFeatures[tweetId][\"place\"] = tweet[\"place\"]\n",
    "        keralaRumoursFeatures[tweetId][\"profileLocation\"] = user[\"location\"]\n",
    "\n",
    "\n",
    "         ################################################################ Syntactical Features ################################################################\n",
    "\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"characterCount\"] = len(rawText)\n",
    "        keralaRumoursFeatures[tweetId][\"tokenCount\"] = len(spacyText)\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        keralaRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        keralaRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "        ### Twitter special tree for future\n",
    "        try:\n",
    "            parser=nlp2.parse(overProcessedText) \n",
    "            tree=Tree.fromstring(parser.__str__()) \n",
    "            keralaRumoursFeatures[tweetId][\"tweetComplexity\"] = tree.height() \n",
    "        except:\n",
    "            keralaRumoursFeatures[tweetId][\"tweetComplexity\"] = 0\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        tags = [x.tag_ for x in spacyText]    \n",
    "        for tag in tags:\n",
    "            keralaRumoursFeatures[tweetId][tag] = tags.count(tag)\n",
    "\n",
    "        ners = [x.ent_type_ for x in spacyText]    \n",
    "        for ner in ners:\n",
    "            if ner != '':\n",
    "                keralaRumoursFeatures[tweetId][ner] = ners.count(ner)\n",
    "\n",
    "         ############################################################### Rumour Language Features ################################################################\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i see\" in overProcessedText else False\n",
    "        keralaRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i hear\" in overProcessedText else False\n",
    "\n",
    "        uppercaseCharCount = sum(1 for i in semiRawText if i.isupper())\n",
    "        lowercaseCharCount = sum(1 for i in semiRawText if i.islower())\n",
    "        keralaRumoursFeatures[tweetId][\"upperCaseCount\"] = uppercaseCharCount\n",
    "        keralaRumoursFeatures[tweetId][\"lowerCaseCount\"] = lowercaseCharCount\n",
    "        try: # Becasue of devision by zero error\n",
    "            keralaRumoursFeatures[tweetId][\"upperCaseCharFrac\"] =  uppercaseCharCount / (uppercaseCharCount + lowercaseCharCount)\n",
    "        except:\n",
    "            keralaRumoursFeatures[tweetId][\"upperCaseCharFrac\"] = 0\n",
    "\n",
    "        capitalWordsCount = len([b for b in [i for i in word_tokenize(processedText)] if b.isupper()])\n",
    "        keralaRumoursFeatures[tweetId][\"capitalWordsCount\"] = capitalWordsCount\n",
    "        try: # Because of devision by zero error\n",
    "            keralaRumoursFeatures[tweetId][\"capitalWordFrac\"] = capitalWordsCount / len(spacyText)\n",
    "        except:\n",
    "            keralaRumoursFeatures[tweetId][\"capitalWordFrac\"] = 0\n",
    "        keralaRumoursFeatures[tweetId][\"exclamationMarkCount\"] = overProcessedText.count(\"!\")\n",
    "        keralaRumoursFeatures[tweetId][\"questionMarkCount\"] = overProcessedText.count(\"?\")\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"firstPersonPronounCount\"] = firstPersonPronounCount(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"secondPersonPronounCount\"] = secondPersonPronounCount(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"thirdPersonPronounCount\"] = thirdPersonPronounCount(overProcessedText)\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"vuglarTermsCount\"] = len([a for a in word_tokenize(overProcessedText) if a in vuglarList])\n",
    "        keralaRumoursFeatures[tweetId][\"emoticonCount\"] = len([a for a in word_tokenize(overProcessedText) if a in emotiList])\n",
    "        keralaRumoursFeatures[tweetId][\"abbreviationCount\"] = len([a for a in word_tokenize(overProcessedText) if a in abbrList])\n",
    "        keralaRumoursFeatures[tweetId][\"emojiCount\"] = len([x for x in overProcessedText if x in emoji.UNICODE_EMOJI])\n",
    "\n",
    "        ############################################################### Rumour Psycholinguistic Features ################################################################\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"subjectivityScore\"] = TextBlob(semiRawText).sentiment.subjectivity\n",
    "        keralaRumoursFeatures[tweetId][\"polarityScore\"] = TextBlob(semiRawText).sentiment.polarity    \n",
    "        \n",
    "        #We have \"angry\" feature from LIWC, that is why I changed the name of this feature from anger to angerEmotion\n",
    "        keralaRumoursFeatures[tweetId][\"angerEmotion\"], keralaRumoursFeatures[tweetId][\"anticipation\"], keralaRumoursFeatures[tweetId][\"disgust\"], \\\n",
    "        keralaRumoursFeatures[tweetId][\"fear\"], keralaRumoursFeatures[tweetId][\"joy\"], keralaRumoursFeatures[tweetId][\"sadness\"], \\\n",
    "        keralaRumoursFeatures[tweetId][\"surprise\"], keralaRumoursFeatures[tweetId][\"trust\"] = nrcEmotions(overProcessedText)\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"positiveEmotion\"], keralaRumoursFeatures[tweetId][\"negativeEmotion\"], keralaRumoursFeatures[tweetId][\"neutralEmotion\"],\\\n",
    "        keralaRumoursFeatures[tweetId][\"compoundEmotion\"], keralaRumoursFeatures[tweetId][\"arousalScore\"], keralaRumoursFeatures[tweetId][\"dominanceScore\"] = \\\n",
    "        emotions(semiRawText, overProcessedText)\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"hateSpeech\"], keralaRumoursFeatures[tweetId][\"offensiveLanguage\"], keralaRumoursFeatures[tweetId][\"neitherClasses\"] = hateSpeech(semiRawText)\n",
    "\n",
    "        \n",
    "        keralaRumoursFeatures[tweetId][\"flesch_reading_ease\"] = textstat.flesch_reading_ease(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"smog_index\"] = textstat.smog_index(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"coleman_liau_index\"] = textstat.coleman_liau_index(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"automated_readability_index\"] = textstat.automated_readability_index(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"difficult_words\"] = textstat.difficult_words(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"linsear_write_formula\"] = textstat.linsear_write_formula(overProcessedText)\n",
    "        keralaRumoursFeatures[tweetId][\"gunning_fog\"] = textstat.gunning_fog(overProcessedText)\n",
    "        \n",
    "        \n",
    "        # One of the syllabus library pitfall is that, an empty sentence is one syllabus\n",
    "        # To avoid devision by zero error\n",
    "        try: \n",
    "            keralaRumoursFeatures[tweetId][\"averageWordsyllables\"] = np.average([syllables.estimate(i) for i in word_tokenize(overProcessedText)])\n",
    "        except:\n",
    "            keralaRumoursFeatures[tweetId][\"averageWordsyllables\"] = 0            \n",
    "        \n",
    "         ################################################################ Rumourmonger Features ################################################################\n",
    "\n",
    "        ### popularity/seclusion features ###\n",
    "        keralaRumoursFeatures[tweetId][\"followingCount\"] = user[\"friends_count\"]\n",
    "        keralaRumoursFeatures[tweetId][\"influnece\"] = user[\"followers_count\"]\n",
    "        keralaRumoursFeatures[tweetId][\"userRole\"] = (user[\"followers_count\"]+1)/(user[\"friends_count\"]+1)\n",
    "\n",
    "         ### activity features ###\n",
    "        today = datetime.datetime.now()\n",
    "        accountCreationTime = datetime.datetime.strptime(user[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        keralaRumoursFeatures[tweetId][\"accountAge\"] = (today.date() - accountCreationTime.date()).days    \n",
    "        keralaRumoursFeatures[tweetId][\"totalProfileLikesCount\"] = user[\"favourites_count\"]\n",
    "        keralaRumoursFeatures[tweetId][\"statusCount\"] = user[\"statuses_count\"]\n",
    "        keralaRumoursFeatures[tweetId][\"averageFollowSpeed\"] = user[\"followers_count\"] / keralaRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        keralaRumoursFeatures[tweetId][\"averageBeingFollowedSpeed\"] = user[\"friends_count\"] / keralaRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        keralaRumoursFeatures[tweetId][\"averageLikeSpeed\"] = user[\"favourites_count\"] / keralaRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        keralaRumoursFeatures[tweetId][\"averageStatusSpeed\"] = user[\"statuses_count\"] / keralaRumoursFeatures[tweetId][\"accountAge\"]\n",
    "\n",
    "         ### profile reputation ###\n",
    "        keralaRumoursFeatures[tweetId][\"isVerifiedAccount\"] = user[\"verified\"]  \n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"userDescriptionReputation\"] = (True if len([i for i in credibleAccountsUsa if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"userDescriptionNotoriety\"] = (True if len([i for i in notoriousId if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"userUrlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"userUrlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "\n",
    "         ### Reticency features ###  \n",
    "        keralaRumoursFeatures[tweetId][\"hasProfileLocation\"] = True if user[\"location\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"hasProfilePicture\"] = True if user[\"profile_image_url\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"geoEnabled\"] = True if user[\"geo_enabled\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"hasProfileUrl\"] = True if user[\"url\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"hasProfileDescription\"] = True if user[\"description\"] != None else False    \n",
    "\n",
    "         ### fake identity ###\n",
    "        keralaRumoursFeatures[tweetId][\"screenNameLength\"] = len(user[\"screen_name\"])\n",
    "        keralaRumoursFeatures[tweetId][\"screenNameDigitCount\"] = len([i for i in user[\"screen_name\"] if i in [str(k) for k in range(0,10)]])   \n",
    "        keralaRumoursFeatures[tweetId][\"protectedProfile\"] = True if user[\"protected\"] != None else False\n",
    "        keralaRumoursFeatures[tweetId][\"personNameInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"PERSON\"]) > 0  else False\n",
    "        keralaRumoursFeatures[tweetId][\"organizationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"ORG\"]) > 0  else False\n",
    "        keralaRumoursFeatures[tweetId][\"locationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"LOC\" or x.ent_type_ == \"GPE\"]) > 0  else False\n",
    "\n",
    "        ### evidence availability ###   \n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"urlAvailability\"] = urlCount > 0\n",
    "        keralaRumoursFeatures[tweetId][\"mediaAvailability\"] = mediaCount > 0\n",
    "        keralaRumoursFeatures[tweetId][\"quoteAvailability\"] = qtFlag\n",
    "\n",
    "         ### evidence diversity ###   \n",
    "        keralaRumoursFeatures[tweetId][\"monoSource\"] = singleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        keralaRumoursFeatures[tweetId][\"doubleSource\"] = doubleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        keralaRumoursFeatures[tweetId][\"trippleSource\"] = (urlCount > 0) and (mediaCount > 0) and qtFlag\n",
    "\n",
    "        ### evidence quality ## First hand / second hand\n",
    "        keralaRumoursFeatures[tweetId][\"firstHandSourceCount\"] = urlCount + mediaCount\n",
    "        keralaRumoursFeatures[tweetId][\"secondHandSourceCount\"] = 1 if qtFlag == True else 0\n",
    "\n",
    "        ### evidence credibility    \n",
    "        keralaRumoursFeatures[tweetId][\"urlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        keralaRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        \n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        keralaRumoursFeatures[tweetId][\"quoteReputation\"] = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        keralaRumoursFeatures[tweetId][\"quoteNotoriety\"] = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "         ################################################################ Reach Features ################################################################\n",
    "        keralaRumoursFeatures[tweetId][\"likeCount\"] = tweet[\"favorite_count\"]\n",
    "        keralaRumoursFeatures[tweetId][\"retweetCount\"] = tweet[\"retweet_count\"]\n",
    "\n",
    "         ################################################################ Miscellaneous Features ################################################################\n",
    "        keralaRumoursFeatures[tweetId][\"hashtagCount\"] = len(entities[\"hashtags\"])\n",
    "        keralaRumoursFeatures[tweetId][\"mentionCount\"] = len(entities[\"user_mentions\"])\n",
    "        keralaRumoursFeatures[tweetId][\"freshness\"] = rtFlag\n",
    "\n",
    "    pk.dump(keralaRumoursFeatures, open(f'{featureSerializationAdr}keralaRumoursFeatures_{counter}.pk', \"wb\"))\n",
    "    keralaRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the features as one pandas dataframe\n",
    "keralaRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"keralaRumoursFeatures_\" in i]\n",
    "keralaRumoursFeaturesList = []\n",
    "for i in keralaRumoursFeaturesFolder:\n",
    "    keralaRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "keralaRumoursFeatures = pd.concat([df for df in keralaRumoursFeaturesList])\n",
    "keralaRumoursFeatures[\"id\"] = keralaRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(keralaRumoursFeatures, open(f'{featureSerializationAdr}/keralaRumoursFeaturesWithoutLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "keralaRumoursLIWC = pk.load(open(featureSerializationAdr+\"keralaRumoursLIWC.pk\", \"rb\"))\n",
    "keralaRumoursLIWC = keralaRumoursLIWC.fillna(0).replace(',','.', regex=True).astype(\"float64\")\n",
    "keralaRumoursFeaturesWithLIWC = pd.merge(keralaRumoursFeatures, keralaRumoursLIWC, on=\"id\")\n",
    "pk.dump(keralaRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/keralaRumoursFeaturesWithLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keralaNonRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"keralaNonRumour\" in i]\n",
    "counter = 0\n",
    "keralaNonRumourId = {}\n",
    "keralaNonRumoursFeatures = {}\n",
    "\n",
    "for file in keralaNonRumourFolder:\n",
    "    keralaNonRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(keralaNonRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in keralaNonRumourId:\n",
    "            keralaNonRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            keralaNonRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ NonRumour General Info ################################################################\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId]={}\n",
    "        keralaNonRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        keralaNonRumoursFeatures[tweetId][\"screenName\"]=user[\"screen_name\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"text\"]=text\n",
    "        keralaNonRumoursFeatures[tweetId][\"tweetUrl\"] = \"https://twitter.com/\" + user[\"screen_name\"] + \"/status/\" + str(tweetId)\n",
    "\n",
    "    ##### =>\n",
    "        pattern = re.compile('[>].*[<]')\n",
    "        try:\n",
    "            keralaNonRumoursFeatures[tweetId][\"source\"] = pattern.findall(tweet[\"source\"])[0][1:-1]\n",
    "        except:\n",
    "            keralaNonRumoursFeatures[tweetId][\"source\"] = tweet[\"source\"]\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"tweetPostTime\"] = datetime.datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    #### =>\n",
    "        keralaNonRumoursFeatures[tweetId][\"place\"] = tweet[\"place\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"profileLocation\"] = user[\"location\"]\n",
    "\n",
    "\n",
    "         ############################################################### Syntactical Features ################################################################\n",
    "\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"characterCount\"] = len(rawText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"tokenCount\"] = len(spacyText)\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "        ### Twitter special tree for future\n",
    "        try:\n",
    "            parser=nlp2.parse(overProcessedText) \n",
    "            tree=Tree.fromstring(parser.__str__()) \n",
    "            keralaNonRumoursFeatures[tweetId][\"tweetComplexity\"] = tree.height() \n",
    "        except:\n",
    "            keralaNonRumoursFeatures[tweetId][\"tweetComplexity\"] = 0\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        tags = [x.tag_ for x in spacyText]    \n",
    "        for tag in tags:\n",
    "            keralaNonRumoursFeatures[tweetId][tag] = tags.count(tag)\n",
    "\n",
    "        ners = [x.ent_type_ for x in spacyText]    \n",
    "        for ner in ners:\n",
    "            if ner != '':\n",
    "                keralaNonRumoursFeatures[tweetId][ner] = ners.count(ner)\n",
    "\n",
    "         ############################################################### NonRumour Language Features ################################################################\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i see\" in overProcessedText else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i hear\" in overProcessedText else False\n",
    "\n",
    "        uppercaseCharCount = sum(1 for i in semiRawText if i.isupper())\n",
    "        lowercaseCharCount = sum(1 for i in semiRawText if i.islower())\n",
    "        keralaNonRumoursFeatures[tweetId][\"upperCaseCount\"] = uppercaseCharCount\n",
    "        keralaNonRumoursFeatures[tweetId][\"lowerCaseCount\"] = lowercaseCharCount\n",
    "        try: # Becasue of devision by zero error\n",
    "            keralaNonRumoursFeatures[tweetId][\"upperCaseCharFrac\"] =  uppercaseCharCount / (uppercaseCharCount + lowercaseCharCount)\n",
    "        except:\n",
    "            keralaNonRumoursFeatures[tweetId][\"upperCaseCharFrac\"] = 0\n",
    "\n",
    "        capitalWordsCount = len([b for b in [i for i in word_tokenize(processedText)] if b.isupper()])\n",
    "        keralaNonRumoursFeatures[tweetId][\"capitalWordsCount\"] = capitalWordsCount\n",
    "        try: # Because of devision by zero error\n",
    "            keralaNonRumoursFeatures[tweetId][\"capitalWordFrac\"] = capitalWordsCount / len(spacyText)\n",
    "        except:\n",
    "            keralaNonRumoursFeatures[tweetId][\"capitalWordFrac\"] = 0\n",
    "        keralaNonRumoursFeatures[tweetId][\"exclamationMarkCount\"] = overProcessedText.count(\"!\")\n",
    "        keralaNonRumoursFeatures[tweetId][\"questionMarkCount\"] = overProcessedText.count(\"?\")\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"firstPersonPronounCount\"] = firstPersonPronounCount(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"secondPersonPronounCount\"] = secondPersonPronounCount(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"thirdPersonPronounCount\"] = thirdPersonPronounCount(overProcessedText)\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"vuglarTermsCount\"] = len([a for a in word_tokenize(overProcessedText) if a in vuglarList])\n",
    "        keralaNonRumoursFeatures[tweetId][\"emoticonCount\"] = len([a for a in word_tokenize(overProcessedText) if a in emotiList])\n",
    "        keralaNonRumoursFeatures[tweetId][\"abbreviationCount\"] = len([a for a in word_tokenize(overProcessedText) if a in abbrList])\n",
    "        keralaNonRumoursFeatures[tweetId][\"emojiCount\"] = len([x for x in overProcessedText if x in emoji.UNICODE_EMOJI])\n",
    "\n",
    "        ############################################################### NonRumour Psycholinguistic Features ################################################################\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"subjectivityScore\"] = TextBlob(semiRawText).sentiment.subjectivity\n",
    "        keralaNonRumoursFeatures[tweetId][\"polarityScore\"] = TextBlob(semiRawText).sentiment.polarity    \n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"angerEmotion\"], keralaNonRumoursFeatures[tweetId][\"anticipation\"], keralaNonRumoursFeatures[tweetId][\"disgust\"], \\\n",
    "        keralaNonRumoursFeatures[tweetId][\"fear\"], keralaNonRumoursFeatures[tweetId][\"joy\"], keralaNonRumoursFeatures[tweetId][\"sadness\"], \\\n",
    "        keralaNonRumoursFeatures[tweetId][\"surprise\"], keralaNonRumoursFeatures[tweetId][\"trust\"] = nrcEmotions(overProcessedText)\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"positiveEmotion\"], keralaNonRumoursFeatures[tweetId][\"negativeEmotion\"], keralaNonRumoursFeatures[tweetId][\"neutralEmotion\"],\\\n",
    "        keralaNonRumoursFeatures[tweetId][\"compoundEmotion\"], keralaNonRumoursFeatures[tweetId][\"arousalScore\"], keralaNonRumoursFeatures[tweetId][\"dominanceScore\"] = \\\n",
    "        emotions(semiRawText, overProcessedText)\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"hateSpeech\"], keralaNonRumoursFeatures[tweetId][\"offensiveLanguage\"], keralaNonRumoursFeatures[tweetId][\"neitherClasses\"] = hateSpeech(semiRawText)\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"flesch_reading_ease\"] = textstat.flesch_reading_ease(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"smog_index\"] = textstat.smog_index(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"coleman_liau_index\"] = textstat.coleman_liau_index(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"automated_readability_index\"] = textstat.automated_readability_index(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"difficult_words\"] = textstat.difficult_words(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"linsear_write_formula\"] = textstat.linsear_write_formula(overProcessedText)\n",
    "        keralaNonRumoursFeatures[tweetId][\"gunning_fog\"] = textstat.gunning_fog(overProcessedText)\n",
    "        \n",
    "        \n",
    "        # One of the syllabus library pitfall is that, an empty sentence is one syllabus\n",
    "        # To avoid devision by zero error\n",
    "        try: \n",
    "            keralaNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = np.average([syllables.estimate(i) for i in word_tokenize(overProcessedText)])\n",
    "        except:\n",
    "            keralaNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = 0      \n",
    "         ################################################################ NonRumourmonger Features ################################################################\n",
    "\n",
    "        ### popularity/seclusion features ###\n",
    "        keralaNonRumoursFeatures[tweetId][\"followingCount\"] = user[\"friends_count\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"influnece\"] = user[\"followers_count\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"userRole\"] = (user[\"followers_count\"]+1)/(user[\"friends_count\"]+1)\n",
    "\n",
    "         ### activity features ###\n",
    "        today = datetime.datetime.now()\n",
    "        accountCreationTime = datetime.datetime.strptime(user[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        keralaNonRumoursFeatures[tweetId][\"accountAge\"] = (today.date() - accountCreationTime.date()).days    \n",
    "        keralaNonRumoursFeatures[tweetId][\"totalProfileLikesCount\"] = user[\"favourites_count\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"statusCount\"] = user[\"statuses_count\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageFollowSpeed\"] = user[\"followers_count\"] / keralaNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageBeingFollowedSpeed\"] = user[\"friends_count\"] / keralaNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageLikeSpeed\"] = user[\"favourites_count\"] / keralaNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"averageStatusSpeed\"] = user[\"statuses_count\"] / keralaNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "\n",
    "         ### profile reputation ###\n",
    "        keralaNonRumoursFeatures[tweetId][\"isVerifiedAccount\"] = user[\"verified\"]  \n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"userDescriptionReputation\"] = (True if len([i for i in credibleAccountsUsa if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"userDescriptionNotoriety\"] = (True if len([i for i in notoriousId if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"userUrlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"userUrlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "\n",
    "         ### Reticency features ###  \n",
    "        keralaNonRumoursFeatures[tweetId][\"hasProfileLocation\"] = True if user[\"location\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"hasProfilePicture\"] = True if user[\"profile_image_url\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"geoEnabled\"] = True if user[\"geo_enabled\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"hasProfileUrl\"] = True if user[\"url\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"hasProfileDescription\"] = True if user[\"description\"] != None else False    \n",
    "\n",
    "         ### fake identity ###\n",
    "        keralaNonRumoursFeatures[tweetId][\"screenNameLength\"] = len(user[\"screen_name\"])\n",
    "        keralaNonRumoursFeatures[tweetId][\"screenNameDigitCount\"] = len([i for i in user[\"screen_name\"] if i in [str(k) for k in range(0,10)]])   \n",
    "        keralaNonRumoursFeatures[tweetId][\"protectedProfile\"] = True if user[\"protected\"] != None else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"personNameInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"PERSON\"]) > 0  else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"organizationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"ORG\"]) > 0  else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"locationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"LOC\" or x.ent_type_ == \"GPE\"]) > 0  else False\n",
    "\n",
    "        ### evidence availability ###   \n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"urlAvailability\"] = urlCount > 0\n",
    "        keralaNonRumoursFeatures[tweetId][\"mediaAvailability\"] = mediaCount > 0\n",
    "        keralaNonRumoursFeatures[tweetId][\"quoteAvailability\"] = qtFlag\n",
    "\n",
    "         ### evidence diversity ###   \n",
    "        keralaNonRumoursFeatures[tweetId][\"monoSource\"] = singleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        keralaNonRumoursFeatures[tweetId][\"doubleSource\"] = doubleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        keralaNonRumoursFeatures[tweetId][\"trippleSource\"] = (urlCount > 0) and (mediaCount > 0) and qtFlag\n",
    "\n",
    "        ### evidence quality ## First hand / second hand\n",
    "        keralaNonRumoursFeatures[tweetId][\"firstHandSourceCount\"] = urlCount + mediaCount\n",
    "        keralaNonRumoursFeatures[tweetId][\"secondHandSourceCount\"] = 1 if qtFlag == True else 0\n",
    "\n",
    "        ### evidence credibility    \n",
    "        keralaNonRumoursFeatures[tweetId][\"urlNotoriety\"]  = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        keralaNonRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        keralaNonRumoursFeatures[tweetId][\"quoteReputation\"]  = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        keralaNonRumoursFeatures[tweetId][\"quoteNotoriety\"]  = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "         ################################################################ Reach Features ################################################################\n",
    "        keralaNonRumoursFeatures[tweetId][\"likeCount\"] = tweet[\"favorite_count\"]\n",
    "        keralaNonRumoursFeatures[tweetId][\"retweetCount\"] = tweet[\"retweet_count\"]\n",
    "\n",
    "\n",
    "         ################################################################ Miscellaneous Features ################################################################\n",
    "        keralaNonRumoursFeatures[tweetId][\"hashtagCount\"] = len(entities[\"hashtags\"])\n",
    "        keralaNonRumoursFeatures[tweetId][\"mentionCount\"] = len(entities[\"user_mentions\"])\n",
    "        keralaNonRumoursFeatures[tweetId][\"freshness\"] = rtFlag\n",
    "\n",
    "    pk.dump(keralaNonRumoursFeatures, open(f'{featureSerializationAdr}keralaNonRumoursFeatures_{counter}.pk', \"wb\"))\n",
    "    keralaNonRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the features as one pandas dataframe\n",
    "keralaNonRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"keralaNonRumoursFeatures_\" in i]\n",
    "keralaNonRumoursFeaturesList = []\n",
    "for i in keralaNonRumoursFeaturesFolder:\n",
    "    keralaNonRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "keralaNonRumoursFeatures = pd.concat([df for df in keralaNonRumoursFeaturesList])\n",
    "keralaNonRumoursFeatures[\"id\"] = keralaNonRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(keralaNonRumoursFeatures, open(f'{featureSerializationAdr}/keralaNonRumoursFeaturesWithoutLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "keralaNonRumoursLIWC = pk.load(open(featureSerializationAdr+\"keralaNonRumoursLIWC.pk\", \"rb\"))\n",
    "keralaNonRumoursLIWC = keralaNonRumoursLIWC.fillna(0).replace(',','.', regex=True).astype(\"float64\")\n",
    "keralaNonRumoursFeaturesWithLIWC = pd.merge(keralaNonRumoursFeatures, keralaNonRumoursLIWC, on=\"id\")\n",
    "pk.dump(keralaNonRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/keralaNonRumoursFeaturesWithLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florenceRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"florenceRumour\" in i]\n",
    "counter = 0\n",
    "florenceRumourId = {}\n",
    "florenceRumoursFeatures = {}\n",
    "\n",
    "for file in florenceRumourFolder:\n",
    "    florenceRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(florenceRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in florenceRumourId:\n",
    "            florenceRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            florenceRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ Rumour General Info ################################################################\n",
    "\n",
    "        florenceRumoursFeatures[tweetId]={}\n",
    "        florenceRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        florenceRumoursFeatures[tweetId][\"screenName\"]=user[\"screen_name\"]\n",
    "        florenceRumoursFeatures[tweetId][\"text\"]=text\n",
    "        florenceRumoursFeatures[tweetId][\"tweetUrl\"] = \"https://twitter.com/\" + user[\"screen_name\"] + \"/status/\" + str(tweetId)\n",
    "\n",
    "    ##### =>\n",
    "        pattern = re.compile('[>].*[<]')\n",
    "        try:\n",
    "            florenceRumoursFeatures[tweetId][\"source\"] = pattern.findall(tweet[\"source\"])[0][1:-1]\n",
    "        except:\n",
    "            florenceRumoursFeatures[tweetId][\"source\"] = tweet[\"source\"]\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"tweetPostTime\"] = datetime.datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    #### =>\n",
    "        florenceRumoursFeatures[tweetId][\"place\"] = tweet[\"place\"]\n",
    "        florenceRumoursFeatures[tweetId][\"profileLocation\"] = user[\"location\"]\n",
    "\n",
    "\n",
    "         ################################################################ Syntactical Features ################################################################\n",
    "\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"characterCount\"] = len(rawText)\n",
    "        florenceRumoursFeatures[tweetId][\"tokenCount\"] = len(spacyText)\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        florenceRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        florenceRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "        ### Twitter special tree for future\n",
    "        try:\n",
    "            parser=nlp2.parse(overProcessedText) \n",
    "            tree=Tree.fromstring(parser.__str__()) \n",
    "            florenceRumoursFeatures[tweetId][\"tweetComplexity\"] = tree.height() \n",
    "        except:\n",
    "            florenceRumoursFeatures[tweetId][\"tweetComplexity\"] = 0\n",
    "\n",
    "        tags = [x.tag_ for x in spacyText]    \n",
    "        for tag in tags:\n",
    "            florenceRumoursFeatures[tweetId][tag] = tags.count(tag)\n",
    "\n",
    "        ners = [x.ent_type_ for x in spacyText]    \n",
    "        for ner in ners:\n",
    "            if ner != '':\n",
    "                florenceRumoursFeatures[tweetId][ner] = ners.count(ner)\n",
    "\n",
    "         ############################################################### Rumour Language Features ################################################################\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i see\" in overProcessedText else False\n",
    "        florenceRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i hear\" in overProcessedText else False\n",
    "\n",
    "        uppercaseCharCount = sum(1 for i in semiRawText if i.isupper())\n",
    "        lowercaseCharCount = sum(1 for i in semiRawText if i.islower())\n",
    "        florenceRumoursFeatures[tweetId][\"upperCaseCount\"] = uppercaseCharCount\n",
    "        florenceRumoursFeatures[tweetId][\"lowerCaseCount\"] = lowercaseCharCount\n",
    "        try: # Becasue of devision by zero error\n",
    "            florenceRumoursFeatures[tweetId][\"upperCaseCharFrac\"] =  uppercaseCharCount / (uppercaseCharCount + lowercaseCharCount)\n",
    "        except:\n",
    "            florenceRumoursFeatures[tweetId][\"upperCaseCharFrac\"] = 0\n",
    "\n",
    "        capitalWordsCount = len([b for b in [i for i in word_tokenize(processedText)] if b.isupper()])\n",
    "        florenceRumoursFeatures[tweetId][\"capitalWordsCount\"] = capitalWordsCount\n",
    "        try: # Because of devision by zero error\n",
    "            florenceRumoursFeatures[tweetId][\"capitalWordFrac\"] = capitalWordsCount / len(spacyText)\n",
    "        except:\n",
    "            florenceRumoursFeatures[tweetId][\"capitalWordFrac\"] = 0\n",
    "        florenceRumoursFeatures[tweetId][\"exclamationMarkCount\"] = overProcessedText.count(\"!\")\n",
    "        florenceRumoursFeatures[tweetId][\"questionMarkCount\"] = overProcessedText.count(\"?\")\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"firstPersonPronounCount\"] = firstPersonPronounCount(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"secondPersonPronounCount\"] = secondPersonPronounCount(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"thirdPersonPronounCount\"] = thirdPersonPronounCount(overProcessedText)\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"vuglarTermsCount\"] = len([a for a in word_tokenize(overProcessedText) if a in vuglarList])\n",
    "        florenceRumoursFeatures[tweetId][\"emoticonCount\"] = len([a for a in word_tokenize(overProcessedText) if a in emotiList])\n",
    "        florenceRumoursFeatures[tweetId][\"abbreviationCount\"] = len([a for a in word_tokenize(overProcessedText) if a in abbrList])\n",
    "        florenceRumoursFeatures[tweetId][\"emojiCount\"] = len([x for x in overProcessedText if x in emoji.UNICODE_EMOJI])\n",
    "\n",
    "        ################################################################ Rumour Psycholinguistic Features ################################################################\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"subjectivityScore\"] = TextBlob(semiRawText).sentiment.subjectivity\n",
    "        florenceRumoursFeatures[tweetId][\"polarityScore\"] = TextBlob(semiRawText).sentiment.polarity    \n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"angerEmotion\"], florenceRumoursFeatures[tweetId][\"anticipation\"], florenceRumoursFeatures[tweetId][\"disgust\"], \\\n",
    "        florenceRumoursFeatures[tweetId][\"fear\"], florenceRumoursFeatures[tweetId][\"joy\"], florenceRumoursFeatures[tweetId][\"sadness\"], \\\n",
    "        florenceRumoursFeatures[tweetId][\"surprise\"], florenceRumoursFeatures[tweetId][\"trust\"] = nrcEmotions(overProcessedText)\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"positiveEmotion\"], florenceRumoursFeatures[tweetId][\"negativeEmotion\"], florenceRumoursFeatures[tweetId][\"neutralEmotion\"],\\\n",
    "        florenceRumoursFeatures[tweetId][\"compoundEmotion\"], florenceRumoursFeatures[tweetId][\"arousalScore\"], florenceRumoursFeatures[tweetId][\"dominanceScore\"] = \\\n",
    "        emotions(semiRawText, overProcessedText)\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"hateSpeech\"], florenceRumoursFeatures[tweetId][\"offensiveLanguage\"], florenceRumoursFeatures[tweetId][\"neitherClasses\"] = hateSpeech(semiRawText)\n",
    "        \n",
    "        \n",
    "        florenceRumoursFeatures[tweetId][\"flesch_reading_ease\"] = textstat.flesch_reading_ease(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"smog_index\"] = textstat.smog_index(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"coleman_liau_index\"] = textstat.coleman_liau_index(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"automated_readability_index\"] = textstat.automated_readability_index(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"difficult_words\"] = textstat.difficult_words(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"linsear_write_formula\"] = textstat.linsear_write_formula(overProcessedText)\n",
    "        florenceRumoursFeatures[tweetId][\"gunning_fog\"] = textstat.gunning_fog(overProcessedText)\n",
    "        \n",
    "        \n",
    "        # One of the syllabus library pitfall is that, an empty sentence is one syllabus\n",
    "        # To avoid devision by zero error\n",
    "        try: \n",
    "            florenceRumoursFeatures[tweetId][\"averageWordsyllables\"] = np.average([syllables.estimate(i) for i in word_tokenize(overProcessedText)])\n",
    "        except:\n",
    "            florenceRumoursFeatures[tweetId][\"averageWordsyllables\"] = 0         \n",
    "            \n",
    "            \n",
    "         ################################################################ Rumourmonger Features ################################################################\n",
    "\n",
    "        ### popularity/seclusion features ###\n",
    "        florenceRumoursFeatures[tweetId][\"followingCount\"] = user[\"friends_count\"]\n",
    "        florenceRumoursFeatures[tweetId][\"influnece\"] = user[\"followers_count\"]\n",
    "        florenceRumoursFeatures[tweetId][\"userRole\"] = (user[\"followers_count\"]+1)/(user[\"friends_count\"]+1)\n",
    "\n",
    "         ### activity features ###\n",
    "        today = datetime.datetime.now()\n",
    "        accountCreationTime = datetime.datetime.strptime(user[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        florenceRumoursFeatures[tweetId][\"accountAge\"] = (today.date() - accountCreationTime.date()).days    \n",
    "        florenceRumoursFeatures[tweetId][\"totalProfileLikesCount\"] = user[\"favourites_count\"]\n",
    "        florenceRumoursFeatures[tweetId][\"statusCount\"] = user[\"statuses_count\"]\n",
    "        florenceRumoursFeatures[tweetId][\"averageFollowSpeed\"] = user[\"followers_count\"] / florenceRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        florenceRumoursFeatures[tweetId][\"averageBeingFollowedSpeed\"] = user[\"friends_count\"] / florenceRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        florenceRumoursFeatures[tweetId][\"averageLikeSpeed\"] = user[\"favourites_count\"] / florenceRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        florenceRumoursFeatures[tweetId][\"averageStatusSpeed\"] = user[\"statuses_count\"] / florenceRumoursFeatures[tweetId][\"accountAge\"]\n",
    "\n",
    "         ### profile reputation ###\n",
    "        florenceRumoursFeatures[tweetId][\"isVerifiedAccount\"] = user[\"verified\"]  \n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"userDescriptionReputation\"] = (True if len([i for i in credibleAccountsUsa if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"userDescriptionNotoriety\"] = (True if len([i for i in notoriousId if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"userUrlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"userUrlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "\n",
    "         ### Reticency features ###  \n",
    "        florenceRumoursFeatures[tweetId][\"hasProfileLocation\"] = True if user[\"location\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"hasProfilePicture\"] = True if user[\"profile_image_url\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"geoEnabled\"] = True if user[\"geo_enabled\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"hasProfileUrl\"] = True if user[\"url\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"hasProfileDescription\"] = True if user[\"description\"] != None else False    \n",
    "\n",
    "         ### fake identity ###\n",
    "        florenceRumoursFeatures[tweetId][\"screenNameLength\"] = len(user[\"screen_name\"])\n",
    "        florenceRumoursFeatures[tweetId][\"screenNameDigitCount\"] = len([i for i in user[\"screen_name\"] if i in [str(k) for k in range(0,10)]])   \n",
    "        florenceRumoursFeatures[tweetId][\"protectedProfile\"] = True if user[\"protected\"] != None else False\n",
    "        florenceRumoursFeatures[tweetId][\"personNameInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"PERSON\"]) > 0  else False\n",
    "        florenceRumoursFeatures[tweetId][\"organizationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"ORG\"]) > 0  else False\n",
    "        florenceRumoursFeatures[tweetId][\"locationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"LOC\" or x.ent_type_ == \"GPE\"]) > 0  else False\n",
    "\n",
    "        ### evidence availability ###   \n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"urlAvailability\"] = urlCount > 0\n",
    "        florenceRumoursFeatures[tweetId][\"mediaAvailability\"] = mediaCount > 0\n",
    "        florenceRumoursFeatures[tweetId][\"quoteAvailability\"] = qtFlag\n",
    "\n",
    "         ### evidence diversity ###   \n",
    "        florenceRumoursFeatures[tweetId][\"monoSource\"] = singleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        florenceRumoursFeatures[tweetId][\"doubleSource\"] = doubleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        florenceRumoursFeatures[tweetId][\"trippleSource\"] = (urlCount > 0) and (mediaCount > 0) and qtFlag\n",
    "\n",
    "        ### evidence quality ## First hand / second hand\n",
    "        florenceRumoursFeatures[tweetId][\"firstHandSourceCount\"] = urlCount + mediaCount\n",
    "        florenceRumoursFeatures[tweetId][\"secondHandSourceCount\"] = 1 if qtFlag == True else 0\n",
    "\n",
    "        ### evidence credibility    \n",
    "        florenceRumoursFeatures[tweetId][\"urlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        florenceRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        florenceRumoursFeatures[tweetId][\"quoteReputation\"] = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        florenceRumoursFeatures[tweetId][\"quoteNotoriety\"] = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "         ################################################################ Reach Features ################################################################\n",
    "        florenceRumoursFeatures[tweetId][\"likeCount\"] = tweet[\"favorite_count\"]\n",
    "        florenceRumoursFeatures[tweetId][\"retweetCount\"] = tweet[\"retweet_count\"]\n",
    "\n",
    "         ################################################################ Miscellaneous Features ################################################################\n",
    "        florenceRumoursFeatures[tweetId][\"hashtagCount\"] = len(entities[\"hashtags\"])\n",
    "        florenceRumoursFeatures[tweetId][\"mentionCount\"] = len(entities[\"user_mentions\"])\n",
    "        florenceRumoursFeatures[tweetId][\"freshness\"] = rtFlag\n",
    "\n",
    "    pk.dump(florenceRumoursFeatures, open(f'{featureSerializationAdr}florenceRumoursFeatures_{counter}.pk', \"wb\"))\n",
    "    florenceRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the features as one pandas dataframe\n",
    "florenceRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"florenceRumoursFeatures_\" in i]\n",
    "florenceRumoursFeaturesList = []\n",
    "for i in florenceRumoursFeaturesFolder:\n",
    "    florenceRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "florenceRumoursFeatures = pd.concat([df for df in florenceRumoursFeaturesList])\n",
    "florenceRumoursFeatures[\"id\"] = florenceRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(florenceRumoursFeatures, open(f'{featureSerializationAdr}/florenceRumoursFeaturesWithoutLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "florenceRumoursLIWC = pk.load(open(featureSerializationAdr+\"florenceRumoursLIWC.pk\", \"rb\"))\n",
    "florenceRumoursLIWC = florenceRumoursLIWC.fillna(0).replace(',','.', regex=True).astype(\"float64\")\n",
    "florenceRumoursFeaturesWithLIWC = pd.merge(florenceRumoursFeatures, florenceRumoursLIWC, on=\"id\")\n",
    "pk.dump(florenceRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/florenceRumoursFeaturesWithLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florenceNonRumourFolder = [i for i in os.listdir(tweetSerializationAdr) if \"florenceNonRumour\" in i]\n",
    "counter = 0\n",
    "florenceNonRumourId = {}\n",
    "florenceNonRumoursFeatures = {}\n",
    "\n",
    "for file in florenceNonRumourFolder:\n",
    "    florenceNonRumours = pk.load(open(tweetSerializationAdr+file, \"rb\")) \n",
    "    for tweet in tqdm(florenceNonRumours):\n",
    "    ################################################################ Basic Setup ################################################################\n",
    "        counter += 1\n",
    "        qtText, qtEntities, qtUser, rtUser, tweetText = None, None, None, None, None\n",
    "\n",
    "        elements = tweetElements(tweet)\n",
    "        rtFlag = copy.deepcopy(elements[0])\n",
    "        qtFlag = copy.deepcopy(elements[1]) \n",
    "\n",
    "        if rtFlag == False and qtFlag == False:\n",
    "            text, entities, user = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4])\n",
    "        elif rtFlag == True and qtFlag == False:\n",
    "            text, entities, user, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5])\n",
    "        elif rtFlag == False and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7])       \n",
    "        elif rtFlag == True and qtFlag == True:\n",
    "            text, entities, user, qtText, qtEntities, qtUser, rtUser = copy.deepcopy(elements[2]), copy.deepcopy(elements[3]), copy.deepcopy(elements[4]), copy.deepcopy(elements[5]), copy.deepcopy(elements[6]), copy.deepcopy(elements[7]), copy.deepcopy(elements[8])\n",
    "\n",
    "        rawText, semiRawText, processedText, overProcessedText = textProcessor(text)\n",
    "\n",
    "        tweetId = tweet[\"id\"]\n",
    "        name = user[\"name\"].lower().strip()    \n",
    "\n",
    "        spacyText = nlp(overProcessedText) \n",
    "        spacyName = nlp(name)\n",
    "\n",
    "\n",
    "        # There are some duplicated tweets (I don't know why!)\n",
    "        if tweetId in florenceNonRumourId:\n",
    "            florenceNonRumourId[tweetId] += 1\n",
    "            continue\n",
    "        else:\n",
    "            florenceNonRumourId[tweetId] = 1\n",
    "\n",
    "\n",
    "         ################################################################ NonRumour General Info ################################################################\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId]={}\n",
    "        florenceNonRumoursFeatures[tweetId][\"id\"]=tweetId\n",
    "        florenceNonRumoursFeatures[tweetId][\"screenName\"]=user[\"screen_name\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"text\"]=text\n",
    "        florenceNonRumoursFeatures[tweetId][\"tweetUrl\"] = \"https://twitter.com/\" + user[\"screen_name\"] + \"/status/\" + str(tweetId)\n",
    "\n",
    "    ##### =>\n",
    "        pattern = re.compile('[>].*[<]')\n",
    "        try:\n",
    "            florenceNonRumoursFeatures[tweetId][\"source\"] = pattern.findall(tweet[\"source\"])[0][1:-1]\n",
    "        except:\n",
    "            florenceNonRumoursFeatures[tweetId][\"source\"] = tweet[\"source\"]\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"tweetPostTime\"] = datetime.datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    #### =>\n",
    "        florenceNonRumoursFeatures[tweetId][\"place\"] = tweet[\"place\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"profileLocation\"] = user[\"location\"]\n",
    "\n",
    "\n",
    "         ################################################################ Syntactical Features ################################################################\n",
    "\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"characterCount\"] = len(rawText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"tokenCount\"] = len(spacyText)\n",
    "        sentences = sentenceTokenizer.tokenize(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"sentenceCount\"] = len(sentences)\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageWordComplexity\"] = np.average([len(i) for i in word_tokenize(overProcessedText)])\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageSentenceComplexity\"] = np.average([len(word_tokenize(i)) for i in sentences])\n",
    "\n",
    "        ### Twitter special tree for future\n",
    "        try:\n",
    "            parser=nlp2.parse(overProcessedText) \n",
    "            tree=Tree.fromstring(parser.__str__()) \n",
    "            florenceNonRumoursFeatures[tweetId][\"tweetComplexity\"] = tree.height() \n",
    "        except:\n",
    "            florenceNonRumoursFeatures[tweetId][\"tweetComplexity\"] = 0\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        tags = [x.tag_ for x in spacyText]    \n",
    "        for tag in tags:\n",
    "            florenceNonRumoursFeatures[tweetId][tag] = tags.count(tag)\n",
    "\n",
    "        ners = [x.ent_type_ for x in spacyText]    \n",
    "        for ner in ners:\n",
    "            if ner != '':\n",
    "                florenceNonRumoursFeatures[tweetId][ner] = ners.count(ner)\n",
    "\n",
    "         ################################################################ NonRumour Language Features ################################################################\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i see\" in overProcessedText else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"witnessPhrases\"] = True if \"i hear\" in overProcessedText else False\n",
    "\n",
    "        uppercaseCharCount = sum(1 for i in semiRawText if i.isupper())\n",
    "        lowercaseCharCount = sum(1 for i in semiRawText if i.islower())\n",
    "        florenceNonRumoursFeatures[tweetId][\"upperCaseCount\"] = uppercaseCharCount\n",
    "        florenceNonRumoursFeatures[tweetId][\"lowerCaseCount\"] = lowercaseCharCount\n",
    "        try: # Becasue of devision by zero error\n",
    "            florenceNonRumoursFeatures[tweetId][\"upperCaseCharFrac\"] =  uppercaseCharCount / (uppercaseCharCount + lowercaseCharCount)\n",
    "        except:\n",
    "            florenceNonRumoursFeatures[tweetId][\"upperCaseCharFrac\"] = 0\n",
    "\n",
    "        capitalWordsCount = len([b for b in [i for i in word_tokenize(processedText)] if b.isupper()])\n",
    "        florenceNonRumoursFeatures[tweetId][\"capitalWordsCount\"] = capitalWordsCount\n",
    "        try: # Because of devision by zero error\n",
    "            florenceNonRumoursFeatures[tweetId][\"capitalWordFrac\"] = capitalWordsCount / len(spacyText)\n",
    "        except:\n",
    "            florenceNonRumoursFeatures[tweetId][\"capitalWordFrac\"] = 0\n",
    "        florenceNonRumoursFeatures[tweetId][\"exclamationMarkCount\"] = overProcessedText.count(\"!\")\n",
    "        florenceNonRumoursFeatures[tweetId][\"questionMarkCount\"] = overProcessedText.count(\"?\")\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"firstPersonPronounCount\"] = firstPersonPronounCount(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"secondPersonPronounCount\"] = secondPersonPronounCount(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"thirdPersonPronounCount\"] = thirdPersonPronounCount(overProcessedText)\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"vuglarTermsCount\"] = len([a for a in word_tokenize(overProcessedText) if a in vuglarList])\n",
    "        florenceNonRumoursFeatures[tweetId][\"emoticonCount\"] = len([a for a in word_tokenize(overProcessedText) if a in emotiList])\n",
    "        florenceNonRumoursFeatures[tweetId][\"abbreviationCount\"] = len([a for a in word_tokenize(overProcessedText) if a in abbrList])\n",
    "        florenceNonRumoursFeatures[tweetId][\"emojiCount\"] = len([x for x in overProcessedText if x in emoji.UNICODE_EMOJI])\n",
    "\n",
    "        ################################################################ NonRumour Psycholinguistic Features ################################################################\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"subjectivityScore\"] = TextBlob(semiRawText).sentiment.subjectivity\n",
    "        florenceNonRumoursFeatures[tweetId][\"polarityScore\"] = TextBlob(semiRawText).sentiment.polarity    \n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"angerEmotion\"], florenceNonRumoursFeatures[tweetId][\"anticipation\"], florenceNonRumoursFeatures[tweetId][\"disgust\"], \\\n",
    "        florenceNonRumoursFeatures[tweetId][\"fear\"], florenceNonRumoursFeatures[tweetId][\"joy\"], florenceNonRumoursFeatures[tweetId][\"sadness\"], \\\n",
    "        florenceNonRumoursFeatures[tweetId][\"surprise\"], florenceNonRumoursFeatures[tweetId][\"trust\"] = nrcEmotions(overProcessedText)\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"positiveEmotion\"], florenceNonRumoursFeatures[tweetId][\"negativeEmotion\"], florenceNonRumoursFeatures[tweetId][\"neutralEmotion\"],\\\n",
    "        florenceNonRumoursFeatures[tweetId][\"compoundEmotion\"], florenceNonRumoursFeatures[tweetId][\"arousalScore\"], florenceNonRumoursFeatures[tweetId][\"dominanceScore\"] = \\\n",
    "        emotions(semiRawText, overProcessedText)\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"hateSpeech\"], florenceNonRumoursFeatures[tweetId][\"offensiveLanguage\"], florenceNonRumoursFeatures[tweetId][\"neitherClasses\"] = hateSpeech(semiRawText)\n",
    "        \n",
    "        \n",
    "        florenceNonRumoursFeatures[tweetId][\"flesch_reading_ease\"] = textstat.flesch_reading_ease(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"smog_index\"] = textstat.smog_index(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"coleman_liau_index\"] = textstat.coleman_liau_index(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"automated_readability_index\"] = textstat.automated_readability_index(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"difficult_words\"] = textstat.difficult_words(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"linsear_write_formula\"] = textstat.linsear_write_formula(overProcessedText)\n",
    "        florenceNonRumoursFeatures[tweetId][\"gunning_fog\"] = textstat.gunning_fog(overProcessedText)\n",
    "        \n",
    "        \n",
    "        # One of the syllabus library pitfall is that, an empty sentence is one syllabus\n",
    "        # To avoid devision by zero error\n",
    "        try: \n",
    "            florenceNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = np.average([syllables.estimate(i) for i in word_tokenize(overProcessedText)])\n",
    "        except:\n",
    "            florenceNonRumoursFeatures[tweetId][\"averageWordsyllables\"] = 0   \n",
    "         ################################################################ NonRumourmonger Features ################################################################\n",
    "\n",
    "        ### popularity/seclusion features ###\n",
    "        florenceNonRumoursFeatures[tweetId][\"followingCount\"] = user[\"friends_count\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"influnece\"] = user[\"followers_count\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"userRole\"] = (user[\"followers_count\"]+1)/(user[\"friends_count\"]+1)\n",
    "\n",
    "         ### activity features ###\n",
    "        today = datetime.datetime.now()\n",
    "        accountCreationTime = datetime.datetime.strptime(user[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        florenceNonRumoursFeatures[tweetId][\"accountAge\"] = (today.date() - accountCreationTime.date()).days    \n",
    "        florenceNonRumoursFeatures[tweetId][\"totalProfileLikesCount\"] = user[\"favourites_count\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"statusCount\"] = user[\"statuses_count\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageFollowSpeed\"] = user[\"followers_count\"] / florenceNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageBeingFollowedSpeed\"] = user[\"friends_count\"] / florenceNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageLikeSpeed\"] = user[\"favourites_count\"] / florenceNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"averageStatusSpeed\"] = user[\"statuses_count\"] / florenceNonRumoursFeatures[tweetId][\"accountAge\"]\n",
    "\n",
    "         ### profile reputation ###\n",
    "        florenceNonRumoursFeatures[tweetId][\"isVerifiedAccount\"] = user[\"verified\"]  \n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"userDescriptionReputation\"] = (True if len([i for i in credibleAccountsUsa if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"userDescriptionNotoriety\"] = (True if len([i for i in notoriousId if i in user[\"description\"].lower()]) > 0 else False) if user[\"description\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"userUrlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"userUrlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in user[\"url\"].lower()]) > 0 else False) if user[\"url\"] != None else False\n",
    "\n",
    "         ### Reticency features ###  \n",
    "        florenceNonRumoursFeatures[tweetId][\"hasProfileLocation\"] = True if user[\"location\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"hasProfilePicture\"] = True if user[\"profile_image_url\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"geoEnabled\"] = True if user[\"geo_enabled\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"hasProfileUrl\"] = True if user[\"url\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"hasProfileDescription\"] = True if user[\"description\"] != None else False    \n",
    "\n",
    "         ### fake identity ###\n",
    "        florenceNonRumoursFeatures[tweetId][\"screenNameLength\"] = len(user[\"screen_name\"])\n",
    "        florenceNonRumoursFeatures[tweetId][\"screenNameDigitCount\"] = len([i for i in user[\"screen_name\"] if i in [str(k) for k in range(0,10)]])   \n",
    "        florenceNonRumoursFeatures[tweetId][\"protectedProfile\"] = True if user[\"protected\"] != None else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"personNameInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"PERSON\"]) > 0  else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"organizationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"ORG\"]) > 0  else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"locationInProfile\"] = True if len([x for x in spacyName if x.ent_type_ == \"LOC\" or x.ent_type_ == \"GPE\"]) > 0  else False\n",
    "\n",
    "        ### evidence availability ###   \n",
    "        urlCount = len(entities[\"urls\"]) if \"urls\" in entities.keys() else 0\n",
    "        mediaCount = len(entities[\"media\"]) if \"media\" in entities.keys() else 0\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"urlAvailability\"] = urlCount > 0\n",
    "        florenceNonRumoursFeatures[tweetId][\"mediaAvailability\"] = mediaCount > 0\n",
    "        florenceNonRumoursFeatures[tweetId][\"quoteAvailability\"] = qtFlag\n",
    "\n",
    "         ### evidence diversity ###   \n",
    "        florenceNonRumoursFeatures[tweetId][\"monoSource\"] = singleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        florenceNonRumoursFeatures[tweetId][\"doubleSource\"] = doubleTruth(urlCount > 0, mediaCount > 0, qtFlag)\n",
    "        florenceNonRumoursFeatures[tweetId][\"trippleSource\"] = (urlCount > 0) and (mediaCount > 0) and qtFlag\n",
    "\n",
    "        ### evidence quality ## First hand / second hand\n",
    "        florenceNonRumoursFeatures[tweetId][\"firstHandSourceCount\"] = urlCount + mediaCount\n",
    "        florenceNonRumoursFeatures[tweetId][\"secondHandSourceCount\"] = 1 if qtFlag == True else 0\n",
    "\n",
    "        ### evidence credibility    \n",
    "        florenceNonRumoursFeatures[tweetId][\"urlNotoriety\"] = (True if len([i for i in notoriousWebsites if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "        florenceNonRumoursFeatures[tweetId][\"urlReputation\"] = (True if len([i for i in credibleWebsitesUsa if i in entities[\"urls\"][0][\"expanded_url\"]]) > 0 else False) if urlCount>0 else False\n",
    "\n",
    "        quoteUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlReputation = (len([i for i in credibleWebsitesUsa if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescReputation = (len([i for i in credibleAccountsUsa if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        quoteUrlNotoriety = (len([i for i in notoriousWebsites if i in qtEntities[\"urls\"][0][\"expanded_url\"]]) > 0 if len(qtEntities[\"urls\"])>0 else False) if qtFlag else False\n",
    "        quoteProfileUrlNotoriety = (len([i for i in notoriousWebsites if i in qtUser[\"url\"]]) > 0 if qtUser[\"url\"] != None else False) if qtFlag else False\n",
    "        quoteProfileDescNotoriety = (len([i for i in notoriousId if i in qtUser[\"description\"]]) > 0 if qtUser[\"description\"] != None else False) if qtFlag else False\n",
    "\n",
    "        florenceNonRumoursFeatures[tweetId][\"quoteReputation\"] = quoteUrlReputation or quoteProfileUrlReputation or quoteProfileDescReputation\n",
    "        florenceNonRumoursFeatures[tweetId][\"quoteNotoriety\"] = quoteUrlNotoriety or quoteProfileUrlNotoriety or quoteProfileDescNotoriety\n",
    "\n",
    "         ############################################################### Reach Features ################################################################\n",
    "        florenceNonRumoursFeatures[tweetId][\"likeCount\"] = tweet[\"favorite_count\"]\n",
    "        florenceNonRumoursFeatures[tweetId][\"retweetCount\"] = tweet[\"retweet_count\"]\n",
    "\n",
    "         ################################################################ Miscellaneous Features ################################################################\n",
    "        florenceNonRumoursFeatures[tweetId][\"hashtagCount\"] = len(entities[\"hashtags\"])\n",
    "        florenceNonRumoursFeatures[tweetId][\"mentionCount\"] = len(entities[\"user_mentions\"])\n",
    "        florenceNonRumoursFeatures[tweetId][\"freshness\"] = rtFlag\n",
    "\n",
    "    pk.dump(florenceNonRumoursFeatures, open(f'{featureSerializationAdr}florenceNonRumoursFeatures_{counter}.pk', \"wb\"))\n",
    "    florenceNonRumoursFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the features as one pandas dataframe\n",
    "florenceNonRumoursFeaturesFolder = [i for i in os.listdir(featureSerializationAdr) if \"florenceNonRumoursFeatures_\" in i]\n",
    "florenceNonRumoursFeaturesList = []\n",
    "for i in florenceNonRumoursFeaturesFolder:\n",
    "    florenceNonRumoursFeaturesList.append(pd.DataFrame.from_dict(pk.load(open(featureSerializationAdr+i, \"rb\"))).T)\n",
    "florenceNonRumoursFeatures = pd.concat([df for df in florenceNonRumoursFeaturesList])\n",
    "florenceNonRumoursFeatures[\"id\"] = florenceNonRumoursFeatures[\"id\"].astype(\"int64\")\n",
    "pk.dump(florenceNonRumoursFeatures, open(f'{featureSerializationAdr}/florenceNonRumoursFeaturesWithoutLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LIWC features\n",
    "florenceNonRumoursLIWC = pk.load(open(featureSerializationAdr+\"florenceNonRumoursLIWC.pk\", \"rb\"))\n",
    "florenceNonRumoursLIWC = florenceNonRumoursLIWC.fillna(0).replace(',','.', regex=True).astype(\"float64\")\n",
    "florenceNonRumoursFeaturesWithLIWC = pd.merge(florenceNonRumoursFeatures, florenceNonRumoursLIWC, on=\"id\")\n",
    "pk.dump(florenceNonRumoursFeaturesWithLIWC, open(f'{featureSerializationAdr}/florenceNonRumoursFeaturesWithLIWC.pk', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_dict(pk.load(open( './Serialization/Features/florenceRumoursFeatures_50000' ,\"rb\")), orient=\"index\")\n",
    "df2 = pd.DataFrame.from_dict(pk.load(open( './Serialization/Features/florenceRumoursFeatures_100000' ,\"rb\")), orient=\"index\")\n",
    "df3 = pd.DataFrame.from_dict(pk.load(open( './Serialization/Features/florenceRumoursFeatures_119889' ,\"rb\")), orient=\"index\")\n",
    "df4 = pd.DataFrame.from_dict(pk.load(open( './Serialization/Features/florenceRumoursFeatures_169889' ,\"rb\")), orient=\"index\")\n",
    "df = pd.concat([df1,df2,df3,df4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
