{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pk\n",
    "import copy\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import io\n",
    "from scipy import misc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid \n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import naive_bayes \n",
    "from sklearn import neural_network\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "import sklearn.metrics as mt \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some address\n",
    "processedFeatureSerializationAdr = \"./Serialization/Features/Processed features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the features\n",
    "kn = pk.load(open(processedFeatureSerializationAdr+\"keralaNonRumours_4thOrderPreprocessing.pk\",\"rb\"))\n",
    "kr = pk.load(open(processedFeatureSerializationAdr+\"keralaRumours_4thOrderPreprocessing.pk\",\"rb\"))\n",
    "fn = pk.load(open(processedFeatureSerializationAdr+\"florenceNonRumours_4thOrderPreprocessing.pk\",\"rb\"))\n",
    "fr = pk.load(open(processedFeatureSerializationAdr+\"florenceRumours_4thOrderPreprocessing.pk\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning labels\n",
    "\n",
    "kr[\"label\"] = 1\n",
    "kn[\"label\"] = 0\n",
    "fr[\"label\"] = 1\n",
    "fn[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the rumour and non-rumours and separating their labels from the data\n",
    "Xf = shuffle(pd.concat([fr,fn]))\n",
    "yf = Xf[\"label\"]\n",
    "Xf = Xf.drop(columns=[\"label\"], axis=\"columns\")\n",
    "\n",
    "Xk = shuffle(pd.concat([kr,kn]))\n",
    "yk = Xk[\"label\"]\n",
    "Xk = Xk.drop(columns=[\"label\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spliting to train, test, and validation\n",
    "# # We do not use it for now ... \n",
    "\n",
    "# X_train_test_f, X_valid_f, y_train_test_f, y_valid_f = train_test_split(Xf, yf, test_size=0.33, random_state=42)\n",
    "# X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_train_test_f, y_train_test_f, test_size=0.33, random_state=42)\n",
    "\n",
    "# X_train_test_k, X_valid_k, y_train_test_k, y_valid_k = train_test_split(Xk, yk, test_size=0.33, random_state=42)\n",
    "# X_train_k, X_test_k, y_train_k, y_test_k = train_test_split(X_train_test_k, y_train_test_k, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "hyperParams = {}\n",
    "hyperParams.update({\"bnb\":{\"alpha\":np.arange(0,1.1,0.1), \"binarize\":[], \"fit_prior\":[True, False]}})\n",
    "hyperParams.update({\"gnb\":{\"priors\":[], \"var_smoothing\":[]}})\n",
    "hyperParams.update({\"mnb\":{\"alpha\":[], \"fit_prior\":[], \"class_prior\":[]}})\n",
    "hyperParams.update({\"cnb\":{\"alpha\":[], \"fit_prior\":[], \"class_prior\":[], \"norm\":[]}})\n",
    "\n",
    "hyperParams.update({\"palm\":{}})\n",
    "hyperParams.update({\"lrlm\":{}})\n",
    "hyperParams.update({\"rlm\":{}})\n",
    "hyperParams.update({\"sgdlm\":{}})\n",
    "\n",
    "hyperParams.update({\"lsvm\":{}})\n",
    "hyperParams.update({\"nusvm\":{}})\n",
    "hyperParams.update({\"csvm\":{}})\n",
    "\n",
    "hyperParams.update({\"dt\":{}})\n",
    "hyperParams.update({\"etdt\":{}})\n",
    "\n",
    "hyperParams.update({\"bnn\":{}})\n",
    "hyperParams.update({\"mlpnn\":{}})\n",
    "\n",
    "hyperParams.update({\"knn\":{}})\n",
    "hyperParams.update({\"rknn\":{}})\n",
    "\n",
    "hyperParams.update({\"bgmm\":{}})\n",
    "hyperParams.update({\"gmm\":{}})\n",
    "hyperParams.update({\"gp\":{}})\n",
    "\n",
    "hyperParams.update({\"ada\":{}})\n",
    "hyperParams.update({\"bag\":{}})\n",
    "hyperParams.update({\"ete\":{}})\n",
    "hyperParams.update({\"gb\":{}})\n",
    "hyperParams.update({\"iso\":{}})\n",
    "hyperParams.update({\"rf\":{}})\n",
    "hyperParams.update({\"rt\":{}})\n",
    "hyperParams.update({\"vot\":{}})\n",
    "hyperParams.update({\"his\":{}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "\n",
    "bnb = naive_bayes.BernoulliNB()\n",
    "gnb = naive_bayes.GaussianNB()\n",
    "mnb = naive_bayes.MultinomialNB()\n",
    "cnb = naive_bayes.ComplementNB()\n",
    "\n",
    "prior= 0.5\n",
    "\n",
    "palm = linear_model.PassiveAggressiveClassifier()\n",
    "lrlm = linear_model.LogisticRegression()\n",
    "sgdlm = linear_model.SGDClassifier()\n",
    "\n",
    "csvm = svm.SVC()\n",
    "kernel: 0:3:0.2\n",
    "c: 0:10:1\n",
    "\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "etdt = tree.ExtraTreeClassifier()\n",
    "\n",
    "bnn = neural_network.BernoulliRBM()\n",
    "mlpnn = neural_network.MLPClassifier()\n",
    "\n",
    "activation:tanh\n",
    "learning rate: adaptive\n",
    "    \n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "rknn = neighbors.RadiusNeighborsClassifier()\n",
    "\n",
    "bgmm = mixture.BayesianGaussianMixture()\n",
    "gmm = mixture.GaussianMixture()\n",
    "n_components: 2:5:1\n",
    "gp = gaussian_process.GaussianProcessClassifier()\n",
    "kernel obkect\n",
    "\n",
    "ada = ensemble.AdaBoostClassifier()\n",
    "bag = ensemble.BaggingClassifier()\n",
    "ete = ensemble.ExtraTreesClassifier()\n",
    "gb = ensemble.GradientBoostingClassifier()\n",
    "iso = ensemble.IsolationForest()\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "rt = ensemble.RandomTreesEmbedding()\n",
    "n_estimator\n",
    "vot = ensemble.VotingClassifier()\n",
    "his = ensemble.HistGradientBoostingClassifier()\n",
    "\n",
    "models = {bnb : \"bnb\" , gnb : \"gnb\" , mnb : \"mnb\" , cnb : \"cnb\" , palm : \"palm\" , # lrlm : \"# lrlm\" ,\\\n",
    "          lrlm : \"lrlm\" , rlm : \"rlm\" , sgdlm : \"sgdlm\" , lsvm : \"lsvm\" , nusvm : \"nusvm\" , \\\n",
    "          csvm : \"csvm\" , dt : \"dt\" , etdt : \"etdt\" , bnn : \"bnn\" , mlpnn : \"mlpnn\" , knn : \"knn\" ,\\\n",
    "          rknn : \"rknn\" , bgmm : \"bgmm\" , gmm : \"gmm\" , gp : \"gp\" , ada : \"ada\" , bag : \"bag\" , ete : \"ete\" ,\\\n",
    "          gb : \"gb\" , iso : \"iso\" , rf : \"rf\" , rt : \"rt\" , vot : \"vot\" , his : \"his\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with kerala and test with florence\n",
    "\n",
    "results = {}\n",
    "for model,modelName in models.items():\n",
    "    params = ParameterGrid(hyperParams[modelName])\n",
    "    \n",
    "    best_score = -1\n",
    "\n",
    "    results[modelName] = {}\n",
    "    t1 = datetime.now()\n",
    "    for g in params:\n",
    "        model.set_params(**g)\n",
    "        model.fit(X_train_k, y_train_k)\n",
    "        precision, recall, thresholds = mt.precision_recall_curve(y_train_k, model.predict_proba(X_train_k)[:,1])\n",
    "        auprc = auc(recall, precision)\n",
    "        if  auprc > best_score:\n",
    "            best_score = auprc\n",
    "            best_grid = g\n",
    "            \n",
    "    t2 = datetime.now()        \n",
    "    model.set_params(**best_grid)\n",
    "    model.fit(X_train_k, y_train_k)\n",
    "    t3 = datetime.now()\n",
    "    y_pred = model.predict(X_test_f)\n",
    "    t4 = datetime.now()\n",
    "    \n",
    "    tn, fp, fn, tp = mt.confusion_matrix(y_test_f, y_pred).ravel()\n",
    "    pr = mt.precision_score(y_test_f, y_pred)\n",
    "    re = mt.recall_score(y_test_f, y_pred)\n",
    "    ac = mt.accuracy_score(y_test_f, y_pred)\n",
    "    f1 = mt.f1_score(y_test_f, y_pred)\n",
    "    precision, recall, thresholds = mt.precision_recall_curve(y_train_k, model.predict_proba(X_train_k)[:,1])\n",
    "    auprc = auc(recall, precision)\n",
    "    auroc = mt.roc_auc_score(y_test_f, y_pred)\n",
    "    \n",
    "    \n",
    "    results[modelName][\"t1\"] = t1\n",
    "    results[modelName][\"t2\"] = t2\n",
    "    results[modelName][\"t3\"] = t3\n",
    "    results[modelName][\"t4\"] = t4\n",
    "    \n",
    "    results[modelName][\"tn\"] = tn\n",
    "    results[modelName][\"fp\"] = fp\n",
    "    results[modelName][\"fn\"] = fn \n",
    "    results[modelName][\"tp\"] = tp\n",
    "    \n",
    "    results[modelName][\"pr\"] = pr \n",
    "    results[modelName][\"re\"] = re\n",
    "    results[modelName][\"f1\"] = f1\n",
    "    results[modelName][\"ac\"] = ac\n",
    "    \n",
    "    results[modelName][\"auprc\"] = auprc \n",
    "    results[modelName][\"auroc\"] = auroc\n",
    "\n",
    "    results[\"bestParameters\"] = best_grid\n",
    "    results[\"model\"] = model\n",
    "\n",
    "pk.dump(results, open(f'./Serialization/Results/{results}_trainWithKerala_balanced.pk', \"wb\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_features\": np.arange(1,10,1)}\n",
    "params = ParameterGrid(parameters)\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "best_score = 0.0\n",
    "\n",
    "for g in params:\n",
    "    dt.set_params(**g)\n",
    "    dt.fit(X_train_k, y_train_k)\n",
    "    f1 = float(mt.f1_score(y_train_k, dt.predict(X_train_k)))\n",
    "    print(f1)\n",
    "    if  f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(dt, param_grid=parameters, cv=1)\n",
    "clf.fit(X_train_k, y_train_k)\n",
    "\n",
    "\n",
    "\n",
    "for g in ParameterGrid(grid):\n",
    "    rf.set_params(**g)\n",
    "    rf.fit(X,y)\n",
    "    # save if best\n",
    "    if rf.oob_score_ > best_score:\n",
    "        best_score = rf.oob_score_\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with Florence / Test with Florence\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = svm.SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "knn = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "clf_2 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "nameDict = {dt:\"Tree\", clf:\"SVM\", rf:\"random forest\", gnb:\"NB\", mlp:\"MLP\", mnb:\"MNB\", ada:\"ADA\", bnb:\"BNB\", clf_2:\"penalize\", lr:\"logistic regression\"}\n",
    "\n",
    "\n",
    "classifiers = [dt, lr, rf, gnb, bnb, mlp, mnb, ada, clf]\n",
    "\n",
    "\n",
    "for model in classifiers:\n",
    "    t1 = datetime.now()\n",
    "    model.fit(X_train_f, y_train_f)\n",
    "    t2 = datetime.now()\n",
    "    y_pred = model.predict(X_test_k)\n",
    "    t3 = datetime.now()\n",
    "    tn, fp, fn, tp = mt.confusion_matrix(y_test_k, y_pred).ravel()\n",
    "    print(\"-------\", nameDict[model], \"-------\")\n",
    "    print(mt.f1_score(y_test_k, y_pred))\n",
    "    print(\"training time: \", (t2-t1).seconds)\n",
    "    print(\"training time: \", (t3-t2).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = mt.confusion_matrix(y_test_k, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------\", nameDict[model], \"-------\")\n",
    "print(mt.f1_score(y_test_k, y_pred))\n",
    "print(\"training time: \", (t2-t1).seconds)\n",
    "print(\"training time: \", (t3-t2).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_pred = model.predict(X_test_k)\n",
    "    t3 = datetime.now()\n",
    "    tn, fp, fn, tp = mt.confusion_matrix(y_test_k, y_pred).ravel()\n",
    "    print(\"-------\", nameDict[model], \"-------\")\n",
    "    print(mt.f1_score(y_test_k, y_pred))\n",
    "    print(\"training time: \", (t2-t1).seconds)\n",
    "    print(\"training time: \", (t3-t2).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "clf = svm.SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "knn = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "clf_2 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "nameDict = {dt:\"Tree\", clf:\"SVM\", rf:\"random forest\", gnb:\"NB\", mlp:\"MLP\", mnb:\"MNB\", ada:\"ADA\", bnb:\"BNB\", clf_2:\"penalize\", lr:\"logistic regression\"}\n",
    "\n",
    "\n",
    "classifiers = [dt, lr, rf, gnb, bnb, mlp, mnb, ada, clf]\n",
    "\n",
    "\n",
    "for model in classifiers:\n",
    "    t1 = datetime.now()\n",
    "    model.fit(X_train_f, y_train_f)\n",
    "    t2 = datetime.now()\n",
    "    y_pred = dt.predict(X_test_f)\n",
    "    t3 = datetime.now()\n",
    "    tn, fp, fn, tp = mt.confusion_matrix(y_test_f, y_pred).ravel()\n",
    "    print(\"-------\", nameDict[model], \"-------\")\n",
    "    print(mt.f1_score(y_test_f, y_pred))\n",
    "    print(\"training time: \", (t2-t1).seconds)\n",
    "    print(\"training time: \", (t3-t2).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments for Zubiaga\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = svm.SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "knn = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "clf_2 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "nameDict = {dt:\"Tree\", clf:\"SVM\", rf:\"random forest\", gnb:\"NB\", mlp:\"MLP\", mnb:\"MNB\", ada:\"ADA\", bnb:\"BNB\", clf_2:\"penalize\", lr:\"logistic regression\"}\n",
    "\n",
    "\n",
    "classifiers = [lr,dt, rf, gnb, bnb, mlp, mnb, ada, clf]\n",
    "\n",
    "\n",
    "for model in classifiers:\n",
    "    kf =  KFold(n_splits=3)\n",
    "    for train_index, test_index in kf.split(zubiaga):\n",
    "        X_Z = zubiaga.iloc[train_index].drop(columns=[\"label\"])\n",
    "        Y_Z = zubiaga.iloc[train_index][\"label\"]\n",
    "        _X_Z = zubiaga.iloc[test_index].drop(columns=[\"label\"])\n",
    "        _Y_Z = zubiaga.iloc[test_index][\"label\"]\n",
    "        \n",
    "        X_train = X_Z\n",
    "        Y_train = Y_Z\n",
    "        X_test = _X_Z\n",
    "        Y_test = _Y_Z\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        tn, fp, fn, tp = mt.confusion_matrix(Y_test, y_pred).ravel()\n",
    "        PR_T = tp/(tp+fp)\n",
    "        RE_T = tp/(tp+fn)\n",
    "        F1_Score = (2*PR_T*RE_T)/(PR_T+RE_T)\n",
    "        ACC_T = (tp+tn)/(tp+fp+tn+fn)\n",
    "        print(nameDict[model], \":\", tp, tn, fp, fn, ACC_T, F1_Score, PR_T, RE_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature selection\n",
    "\n",
    "# # This cell should become active only after doing feature evaluation, because filters variable would be available after feature evaluation\n",
    "# kerala_nr = (pd.DataFrame.from_csv(\"./kerala_nr.csv\"))[filters]\n",
    "# kerala_r = (pd.DataFrame.from_csv(\"./kerala_r.csv\"))[filters]\n",
    "# florence_nr = (pd.DataFrame.from_csv(\"./florence_nr.csv\"))[filters]\n",
    "# florence_r = (pd.DataFrame.from_csv(\"./florence_r.csv\"))[filters]\n",
    "# zubiaga_nr = (pd.DataFrame.from_csv(\"./zubiaga_nr.csv\"))[filters]\n",
    "# zubiaga_r = (pd.DataFrame.from_csv(\"./zubiaga_r.csv\"))[filters] \n",
    "\n",
    "# kerala_nr.to_csv(\"./MATLAB/zubiaga/kerala_nr_filters.csv\") \n",
    "# kerala_r.to_csv(\"./MATLAB/zubiaga/kerala_r_filters.csv\")\n",
    "# florence_nr.to_csv(\"./MATLAB/zubiaga/florence_nr_filters.csv\")\n",
    "# florence_r.to_csv(\"./MATLAB/zubiaga/florence_r_filters.csv\")\n",
    "# zubiaga_nr.to_csv(\"./MATLAB/zubiaga/zubiaga_nr_filters.csv\")\n",
    "# zubiaga_r.to_csv(\"./MATLAB/zubiaga/zubiaga_r_filters.csv\")\n",
    "\n",
    "\n",
    "# kerala_nr = (pd.DataFrame.from_csv(\"./kerala_nr.csv\"))[significantNotOurs]\n",
    "# kerala_r = (pd.DataFrame.from_csv(\"./kerala_r.csv\"))[significantNotOurs]\n",
    "# florence_nr = (pd.DataFrame.from_csv(\"./florence_nr.csv\"))[significantNotOurs]\n",
    "# florence_r = (pd.DataFrame.from_csv(\"./florence_r.csv\"))[significantNotOurs]\n",
    "# zubiaga_nr = (pd.DataFrame.from_csv(\"./zubiaga_nr.csv\"))[significantNotOurs]\n",
    "# zubiaga_r = (pd.DataFrame.from_csv(\"./zubiaga_r.csv\"))[significantNotOurs] \n",
    "\n",
    "\n",
    "# kerala_nr.to_csv(\"./MATLAB/zubiaga/kerala_nr_significantNotOurs.csv\") \n",
    "# kerala_r.to_csv(\"./MATLAB/zubiaga/kerala_r_significantNotOurs.csv\")\n",
    "# florence_nr.to_csv(\"./MATLAB/zubiaga/florence_nr_significantNotOurs.csv\")\n",
    "# florence_r.to_csv(\"./MATLAB/zubiaga/florence_r_significantNotOurs.csv\")\n",
    "# zubiaga_nr.to_csv(\"./MATLAB/zubiaga/zubiaga_nr_significantNotOurs.csv\")\n",
    "# zubiaga_r.to_csv(\"./MATLAB/zubiaga/zubiaga_r_significantNotOurs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerala_nr = (pd.DataFrame.from_csv(\"./kerala_nr.csv\"))\n",
    "kerala_r = (pd.DataFrame.from_csv(\"./kerala_r.csv\"))\n",
    "florence_nr = (pd.DataFrame.from_csv(\"./florence_nr.csv\"))\n",
    "florence_r = (pd.DataFrame.from_csv(\"./florence_r.csv\"))\n",
    "zubiaga_nr = (pd.DataFrame.from_csv(\"./zubiaga_nr.csv\"))\n",
    "zubiaga_r = (pd.DataFrame.from_csv(\"./zubiaga_r.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florence_nr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kerala_nr))\n",
    "print(len(kerala_r))\n",
    "print(len(florence_nr))\n",
    "print(len(florence_r))\n",
    "print(len(zubiaga_nr))\n",
    "print(len(zubiaga_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding rumour/non-rumour label\n",
    "#rumour=1 non-rumour=-1\n",
    "# kerala_r[\"label\"] = pd.Series(1, index=kerala_r.index)\n",
    "# kerala_nr[\"label\"] = pd.Series(-1, index=kerala_nr.index)\n",
    "# florence_r[\"label\"] = pd.Series(1, index=florence_r.index)\n",
    "# florence_nr[\"label\"] = pd.Series(-1, index=florence_nr.index)\n",
    "# zubiaga_r[\"label\"] = pd.Series(1, index=zubiaga_r.index)\n",
    "# zubiaga_nr[\"label\"] = pd.Series(-1, index=zubiaga_nr.index)\n",
    "\n",
    "kerala_r[\"label\"] = 1\n",
    "kerala_nr[\"label\"] = -1\n",
    "florence_r[\"label\"] = 1\n",
    "florence_nr[\"label\"] = -1\n",
    "zubiaga_r[\"label\"] = 1\n",
    "zubiaga_nr[\"label\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = list(kerala_r.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguisticFeatures = [\"exclamationMarkCount\",\"questionMarkCount\",\"characterCount\",\"tokenCount\",\"subjectivity\",\\\n",
    "                      \"polarity\",\"uppercaseCount\",\"lowerCaseCount\",\"firstPersonPronounCount\",\"secondPersonPronounCount\",\\\n",
    "                      \"thirdPersonPronounCount\",\"capitalWordsCount\",\"averageWordComplexity\",\"vuglarTermsCount\",\\\n",
    "                      \"emoticonCount\",\"abbreviationCount\",\"emojiCount\",\"posCoordinatingConjunctionCount\", \"posAdjectiveCount\",\"posAdpositionCount\",\\\n",
    "                      \"posAdverbCount\",\"posAuxiliaryCount\",\"posConjunctionCount\",\"posDeterminerCount\",\\\n",
    "                      \"posInterjectionCount\",\"posNounCount\",\"posNumeralCount\",\"posParticleCount\",\"posPronounCount\",\\\n",
    "                      \"posProperNounCount\",\"posPunctuationCount\",\"posSubordinatingConjunctionCount\",\"posSymbolCount\",\\\n",
    "                      \"posVerbCount\",\"posOtherCount\",\"posSpaceCount\",\"nerPersonCount\",\"nerNationalityCount\",\\\n",
    "                      \"nerBuildingCount\",\"nerOrganizationCount\",\"nerCountriesCount\",\"nerLocationCount\",\"nerProductCount\",\\\n",
    "                      \"nerEventCount\",\"nerArtCount\",\"nerLawCount\",\"nerLanguageCount\",\"nerDateCount\",\"nerTimeCount\",\\\n",
    "                      \"nerMoneyCount\",\"nerQuantityCount\",\"nerOrdinalCount\",\"nerCardinalCount\",\"insight\",\"tentative\",\\\n",
    "                      \"positiveEmotion\",\"negativeEmotion\",\"anxiety\",\"certainty\",\"tone\",\"sentenceComplexity\"]\n",
    "userFeatures = [\"hasProfileDescription\",\"isVerifiedAccount\",\"statusCount\",\"followingCount\",\\\n",
    "                \"influnece\",\"userRole\",\"totalProfileLikesCount\",\"accountAge\",\"protectedProfile\",\\\n",
    "                \"hasProfileLocation\",\"hasProfilePicture\",\"averageFollowSpeed\",\\\n",
    "                \"averageBeingFollowedSpeed\",\"averageLikeSpeed\",\"averageStatusSpeed\",\"screenNameLength\",\\\n",
    "                \"screenNameDigitCount\"]\n",
    "metaFeatures = [\"hashtagCount\",\"mentionCount\",\"hasUrl\",\"geoEnabled\", \"multimediaCounter\"]\n",
    "# , \"tweetPostTime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literatureFeatures = [\"exclamationMarkCount\",\"questionMarkCount\",\"characterCount\",\"tokenCount\",\"subjectivity\",\\\n",
    "                      \"polarity\",\"uppercaseCount\",\"lowerCaseCount\",\"firstPersonPronounCount\",\"secondPersonPronounCount\",\\\n",
    "                      \"thirdPersonPronounCount\",\"capitalWordsCount\",\"averageWordComplexity\",\"vuglarTermsCount\",\\\n",
    "                      \"emoticonCount\",\"abbreviationCount\",\"insight\",\"tentative\",\\\n",
    "                      \"positiveEmotion\",\"negativeEmotion\",\"anxiety\",\"sentenceComplexity\",\\\n",
    "                     \"hasProfileDescription\",\"isVerifiedAccount\",\"statusCount\",\"followingCount\",\\\n",
    "                \"influnece\",\"userRole\", \"accountAge\",\\\n",
    "                \"hasProfileLocation\",\"hashtagCount\",\"mentionCount\",\"hasUrl\",  \"multimediaCounter\"]\n",
    "\n",
    "inspiredFeatures = [\"posCoordinatingConjunctionCount\", \"posAdjectiveCount\",\"posAdpositionCount\",\\\n",
    "                      \"posAdverbCount\",\"posAuxiliaryCount\",\"posConjunctionCount\",\"posDeterminerCount\",\\\n",
    "                      \"posInterjectionCount\",\"posNounCount\",\"posNumeralCount\",\"posParticleCount\",\"posPronounCount\",\\\n",
    "                      \"posProperNounCount\",\"posPunctuationCount\",\"posSubordinatingConjunctionCount\",\"posSymbolCount\",\\\n",
    "                      \"posVerbCount\",\"posOtherCount\",\"posSpaceCount\",\"hasProfilePicture\",\"screenNameLength\",\\\n",
    "                \"screenNameDigitCount\"]\n",
    "\n",
    "developedFeatures = [\"emojiCount\", \"tone\", \"nerPersonCount\",\"nerNationalityCount\",\\\n",
    "                      \"nerBuildingCount\",\"nerOrganizationCount\",\"nerCountriesCount\",\"nerLocationCount\",\"nerProductCount\",\\\n",
    "                      \"nerEventCount\",\"nerArtCount\",\"nerLawCount\",\"nerLanguageCount\",\"nerDateCount\",\"nerTimeCount\",\\\n",
    "                      \"nerMoneyCount\",\"nerQuantityCount\",\"nerOrdinalCount\",\"nerCardinalCount\", \"certainty\", \"totalProfileLikesCount\", \"protectedProfile\",\"averageFollowSpeed\",\\\n",
    "                \"averageBeingFollowedSpeed\",\"averageLikeSpeed\",\"averageStatusSpeed\", \"geoEnabled\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating kerala rumour features by the feature class\n",
    "kerala_r_ling = kerala_r[linguisticFeatures]\n",
    "kerala_r_user = kerala_r[userFeatures]\n",
    "kerala_r_meta = kerala_r[metaFeatures]\n",
    "\n",
    "kerala_r_ling[\"label\"] = 1 \n",
    "kerala_r_user[\"label\"] = 1 \n",
    "kerala_r_meta[\"label\"] = 1 \n",
    "\n",
    "#Separating kerala non rumour features by the feature class\n",
    "kerala_nr_ling = kerala_nr[linguisticFeatures]\n",
    "kerala_nr_user = kerala_nr[userFeatures]\n",
    "kerala_nr_meta = kerala_nr[metaFeatures]\n",
    "\n",
    "kerala_nr_ling[\"label\"] = -1\n",
    "kerala_nr_user[\"label\"] = -1\n",
    "kerala_nr_meta[\"label\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating florence rumour features by the feature class\n",
    "florence_r_ling = florence_r[linguisticFeatures]\n",
    "florence_r_user = florence_r[userFeatures]\n",
    "florence_r_meta = florence_r[metaFeatures]\n",
    "\n",
    "florence_r_ling[\"label\"] = 1\n",
    "florence_r_user[\"label\"] = 1\n",
    "florence_r_meta[\"label\"] = 1\n",
    "\n",
    "#Separating florence non rumour features by the feature class\n",
    "florence_nr_ling = florence_nr[linguisticFeatures]\n",
    "florence_nr_user = florence_nr[userFeatures]\n",
    "florence_nr_meta = florence_nr[metaFeatures]\n",
    "\n",
    "florence_nr_ling[\"label\"] = -1\n",
    "florence_nr_user[\"label\"] = -1\n",
    "florence_nr_meta[\"label\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating zubiaga rumour features by the feature class\n",
    "zubiaga_r_ling = zubiaga_r[linguisticFeatures]\n",
    "zubiaga_r_user = zubiaga_r[userFeatures]\n",
    "zubiaga_r_meta = zubiaga_r[metaFeatures]\n",
    "\n",
    "zubiaga_r_ling[\"label\"] = 1\n",
    "zubiaga_r_user[\"label\"] = 1\n",
    "zubiaga_r_meta[\"label\"] = 1\n",
    "\n",
    "#Separating zubiaga non rumour features by the feature class\n",
    "zubiaga_nr_ling = zubiaga_nr[linguisticFeatures]\n",
    "zubiaga_nr_user = zubiaga_nr[userFeatures]\n",
    "zubiaga_nr_meta = zubiaga_nr[metaFeatures]\n",
    "\n",
    "zubiaga_nr_ling[\"label\"] = -1\n",
    "zubiaga_nr_user[\"label\"] = -1\n",
    "zubiaga_nr_meta[\"label\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature experiment\n",
    "\n",
    "# #\"~averageFollowSpeed\",\"+averageBeingFollowedSpeed\",\"+averageLikeSpeed\",\"averageStatusSpeed\"\n",
    "\n",
    "# florence_nr_without = florence_nr.drop(columns=[\"averageFollowSpeed\",\"averageBeingFollowedSpeed\",\"averageLikeSpeed\",\"averageStatusSpeed\"])\n",
    "# florence_r_without = florence_r.drop(columns=[\"averageFollowSpeed\",\"averageBeingFollowedSpeed\",\"averageLikeSpeed\",\"averageStatusSpeed\"])\n",
    "\n",
    "# florence_nr_with = florence_nr.drop(columns=[\"averageFollowSpeed\", \"averageBeingFollowedSpeed\",\"averageLikeSpeed\"])\n",
    "# florence_r_with = florence_r.drop(columns=[\"averageFollowSpeed\", \"averageBeingFollowedSpeed\",\"averageLikeSpeed\"])\n",
    "\n",
    "# kerala_nr_without = kerala_nr.drop(columns=[\"averageFollowSpeed\",\"averageBeingFollowedSpeed\",\"averageLikeSpeed\",\"averageStatusSpeed\"])\n",
    "# kerala_r_without = kerala_r.drop(columns=[\"averageFollowSpeed\",\"averageBeingFollowedSpeed\",\"averageLikeSpeed\",\"averageStatusSpeed\"])\n",
    "\n",
    "# kerala_nr_with = kerala_nr.drop(columns=[\"averageFollowSpeed\", \"averageBeingFollowedSpeed\",\"averageLikeSpeed\"])\n",
    "# kerala_r_with = kerala_r.drop(columns=[\"averageFollowSpeed\", \"averageBeingFollowedSpeed\",\"averageLikeSpeed\"])\n",
    "\n",
    "\n",
    "\n",
    "# florence_with = pd.concat([florence_nr_with, florence_r_with], ignore_index=True)\n",
    "# florence_without = pd.concat([florence_nr_without, florence_r_without], ignore_index=True)\n",
    "\n",
    "# kerala_with = pd.concat([kerala_nr_with, kerala_r_with], ignore_index=True)\n",
    "# kerala_without = pd.concat([kerala_nr_without, kerala_r_without], ignore_index=True)\n",
    "\n",
    "\n",
    "# Y_Florence_with_pd = florence_with[[\"label\"]]\n",
    "# X_Florence_with_pd = florence_with.drop(columns=[\"label\"])\n",
    "# Y_Florence_without_pd = florence_without[[\"label\"]]\n",
    "# X_Florence_without_pd = florence_without.drop(columns=[\"label\"])\n",
    "\n",
    "\n",
    "# Y_Kerala_with_pd = kerala_with[[\"label\"]]\n",
    "# X_Kerala_with_pd = kerala_with.drop(columns=[\"label\"])\n",
    "# Y_Kerala_without_pd = kerala_without[[\"label\"]]\n",
    "# X_Kerala_without_pd = kerala_without.drop(columns=[\"label\"])\n",
    "\n",
    "\n",
    "# Y_Florence_with = Y_Florence_with_pd.values\n",
    "# X_Florence_with = X_Florence_with_pd.values\n",
    "# Y_Florence_without = Y_Florence_without_pd.values\n",
    "# X_Florence_without = X_Florence_without_pd.values\n",
    "# Y_Kerala_with = Y_Kerala_with_pd.values\n",
    "# X_Kerala_with = X_Kerala_with_pd.values\n",
    "# Y_Kerala_without = Y_Kerala_without_pd.values\n",
    "# X_Kerala_without = X_Kerala_without_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the datas and shuffling them and separating label from training data\n",
    "# Also, making training and test set\n",
    "kerala = pd.concat([kerala_r, kerala_nr], ignore_index=True)\n",
    "florence = pd.concat([florence_r, florence_nr], ignore_index=True)\n",
    "zubiaga = pd.concat([zubiaga_r, zubiaga_nr], ignore_index=True)\n",
    "\n",
    "# kerala_ling = pd.concat([kerala_r_ling, kerala_nr_ling], ignore_index=True)\n",
    "# florence_ling = pd.concat([florence_r_ling, florence_nr_ling], ignore_index=True)\n",
    "# zubiaga_ling = pd.concat([zubiaga_r_ling, zubiaga_nr_ling], ignore_index=True)\n",
    "\n",
    "# kerala_user = pd.concat([kerala_r_user, kerala_nr_user], ignore_index=True)\n",
    "# florence_user = pd.concat([florence_r_user, florence_nr_user], ignore_index=True)\n",
    "# zubiaga_user = pd.concat([zubiaga_r_user, zubiaga_nr_user], ignore_index=True)\n",
    "\n",
    "# kerala_meta = pd.concat([kerala_r_meta, kerala_nr_meta], ignore_index=True)\n",
    "# florence_meta = pd.concat([florence_r_meta, florence_nr_meta], ignore_index=True)\n",
    "# zubiaga_meta = pd.concat([zubiaga_r_meta, zubiaga_nr_meta], ignore_index=True)\n",
    "\n",
    "# kerala = kerala.sample(frac=1)\n",
    "# florence = florence.sample(frac=1)\n",
    "# zubiaga = zubiaga.sample(frac=1)\n",
    "\n",
    "# kerala_ling = kerala_ling.sample(frac=1)\n",
    "# florence_ling = florence_ling.sample(frac=1)\n",
    "# zubiaga_ling = zubiaga_ling.sample(frac=1)\n",
    "\n",
    "# kerala_user = kerala_user.sample(frac=1)\n",
    "# florence_user = florence_user.sample(frac=1)\n",
    "# zubiaga_user = zubiaga_user.sample(frac=1)\n",
    "\n",
    "# kerala_meta = kerala_meta.sample(frac=1)\n",
    "# florence_meta = florence_meta.sample(frac=1)\n",
    "# zubiaga_meta = zubiaga_meta.sample(frac=1)\n",
    "\n",
    "# kerala = kerala.reset_index(drop=True)\n",
    "# florence = florence.reset_index(drop=True)\n",
    "# zubiaga = zubiaga.reset_index(drop=True)\n",
    "# kerala_ling = kerala_ling.reset_index(drop=True)\n",
    "# florence_ling = florence_ling.reset_index(drop=True)\n",
    "# zubiaga_ling = zubiaga_ling.reset_index(drop=True)\n",
    "# kerala_user = kerala_user.reset_index(drop=True)\n",
    "# florence_user = florence_user.reset_index(drop=True)\n",
    "# zubiaga_user = zubiaga_user.reset_index(drop=True)\n",
    "# kerala_meta = kerala_meta.reset_index(drop=True)\n",
    "# florence_meta = florence_meta.reset_index(drop=True)\n",
    "# zubiaga_meta = zubiaga_meta.reset_index(drop=True)\n",
    "\n",
    "Y_Kerala_pd = kerala[[\"label\"]]\n",
    "X_Kerala_pd = kerala.drop(columns=[\"label\"])\n",
    "Y_Florence_pd = florence[[\"label\"]]\n",
    "X_Florence_pd = florence.drop(columns=[\"label\"])\n",
    "Y_Zubiaga_pd = zubiaga[[\"label\"]]\n",
    "X_Zubiaga_pd = zubiaga.drop(columns=[\"label\"])\n",
    "\n",
    "\n",
    "Y_Kerala = Y_Kerala_pd.values\n",
    "X_Kerala = X_Kerala_pd.values\n",
    "Y_Florence = Y_Florence_pd.values\n",
    "X_Florence = X_Florence_pd.values\n",
    "Y_Zubiaga = Y_Zubiaga_pd.values\n",
    "X_Zubiaga = X_Zubiaga_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending datasets to each others\n",
    "# Y_Kerala_Florence_pd = pd.concat([Y_Kerala_pd, Y_Florence_pd], ignore_index=True)\n",
    "# X_Kerala_Florence_pd = pd.concat([X_Kerala_pd, X_Florence_pd], ignore_index=True)\n",
    "# Y_Kerala_Florence_pd.to_csv(\"./MATLAB/kerala_florence_label.csv\")\n",
    "# X_Kerala_Florence_pd.to_csv(\"./MATLAB/kerala_florence_data.csv\")\n",
    "\n",
    "\n",
    "# Y_Kerala_Zubiaga_pd = pd.concat([Y_Kerala_pd, Y_Zubiaga_pd], ignore_index=True)\n",
    "# X_Kerala_Zubiaga_pd = pd.concat([X_Kerala_pd, X_Zubiaga_pd], ignore_index=True)\n",
    "# Y_Kerala_Zubiaga_pd.to_csv(\"./MATLAB/kerala_zubiaga_label.csv\")\n",
    "# X_Kerala_Zubiaga_pd.to_csv(\"./MATLAB/kerala_zubiaga_data.csv\")\n",
    "\n",
    "\n",
    "# Y_Florence_Zubiaga_pd = pd.concat([Y_Florence_pd, Y_Zubiaga_pd], ignore_index=True)\n",
    "# X_Florence_Zubiaga_pd = pd.concat([X_Florence_pd, X_Zubiaga_pd], ignore_index=True)\n",
    "# Y_Florence_Zubiaga_pd.to_csv(\"./MATLAB/florence_zubiaga_label.csv\")\n",
    "# X_Florence_Zubiaga_pd.to_csv(\"./MATLAB/florence_zubiaga_data.csv\")\n",
    "\n",
    "# Y_Kerala_Florence = Y_Kerala_Florence_pd.values\n",
    "# X_Kerala_Florence = X_Kerala_Florence_pd.values\n",
    "# Y_Kerala_Zubiaga = Y_Kerala_Zubiaga_pd.values\n",
    "# X_Kerala_Zubiaga = X_Kerala_Zubiaga_pd.values\n",
    "# Y_Florence_Zubiaga = Y_Florence_Zubiaga_pd.values\n",
    "# X_Florence_Zubiaga = X_Florence_Zubiaga_pd.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "1. The Zubiaga Dataset is the one that is refered to in \"Learning Reporting Dynamics during Breaking News for Rumour Detection in Social Media\"\n",
    "2. The Kerala dataset is collected by me\n",
    "3. The Florence datset is collected by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training set\n",
    "# X : Data  --- Y: Label\n",
    "# For the datasets the the training set and test set are specidied as follows:\n",
    "\n",
    "# Zubiaga Experiment: Train=> X_Z | Y_Z\n",
    "#                     Test=> _X_Z | _Y_Z\n",
    "# Five fold Cross Validation\n",
    "# X_Z = \n",
    "# Y_Z = \n",
    "# _X_Z = \n",
    "# _Y_Z = \n",
    "\n",
    "# Kerala Experiment: Train=> X_K | Y_K\n",
    "#                    Test=> _X_K | _Y_K\n",
    "X_K = X_Kerala\n",
    "Y_K = Y_Kerala\n",
    "_X_K = X_Florence\n",
    "_Y_K = Y_Florence\n",
    "\n",
    "\n",
    "# Florence Experiment: Train=> X_F | Y_F\n",
    "#                      Test=> _X_F | _Y_F\n",
    "\n",
    "X_F = X_Florence\n",
    "Y_F = Y_Florence\n",
    "_X_F = X_Kerala\n",
    "_Y_F = Y_Kerala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments for Zubiaga\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = svm.SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "knn = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "clf_2 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "nameDict = {dt:\"Tree\", clf:\"SVM\", rf:\"random forest\", gnb:\"NB\", mlp:\"MLP\", mnb:\"MNB\", ada:\"ADA\", bnb:\"BNB\", clf_2:\"penalize\", lr:\"logistic regression\"}\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [lr,dt, clf, rf, gnb, bnb, mlp, mnb, ada, clf_2]\n",
    "# classifiers = [dt]\n",
    "\n",
    "for model in classifiers:\n",
    "    zubiaga = zubiaga.sample(frac=1)\n",
    "    kf =  KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(zubiaga):\n",
    "        X_Z = zubiaga.iloc[train_index].drop(columns=[\"label\"])\n",
    "        Y_Z = zubiaga.iloc[train_index][\"label\"]\n",
    "        _X_Z = zubiaga.iloc[test_index].drop(columns=[\"label\"])\n",
    "        _Y_Z = zubiaga.iloc[test_index][\"label\"]\n",
    "        \n",
    "        X_train = X_Z\n",
    "        Y_train = Y_Z\n",
    "        X_test = _X_Z\n",
    "        Y_test = _Y_Z\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        tn, fp, fn, tp = mt.confusion_matrix(Y_test, y_pred).ravel()\n",
    "        PR_T = tp/(tp+fp)\n",
    "        RE_T = tp/(tp+fn)\n",
    "        F1_Score = (2*PR_T*RE_T)/(PR_T+RE_T)\n",
    "        ACC_T = (tp+tn)/(tp+fp+tn+fn)\n",
    "        print(nameDict[model], \":\", tp, tn, fp, fn, ACC_T, F1_Score, PR_T, RE_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments for Florence\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = svm.SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "knn = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "clf_2 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "\n",
    "\n",
    "nameDict = {dt:\"Tree\", clf:\"SVM\", rf:\"random forest\", gnb:\"NB\", mlp:\"MLP\", mnb:\"MNB\", ada:\"ADA\", bnb:\"BNB\", clf_2:\"penalize\"}\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [dt, clf, rf, gnb, bnb, mlp, mnb, ada, clf_2]\n",
    "# classifiers = [clf_2]\n",
    "\n",
    "for model in classifiers:\n",
    "    florence = florence.sample(frac=1)\n",
    "    kf =  KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(florence):\n",
    "\n",
    "        X_F = florence.iloc[train_index].drop(columns=[\"label\"])\n",
    "        Y_F = florence.iloc[train_index][\"label\"]\n",
    "        _X_F = florence.iloc[test_index].drop(columns=[\"label\"])\n",
    "        _Y_F = florence.iloc[test_index][\"label\"]\n",
    "        \n",
    "        X_train = X_F\n",
    "        Y_train = Y_F\n",
    "        X_test = _X_F\n",
    "        Y_test = _Y_F\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        tn, fp, fn, tp = mt.confusion_matrix(Y_test, y_pred).ravel()\n",
    "        PR_T = tp/(tp+fp)\n",
    "        RE_T = tp/(tp+fn)\n",
    "        F1_Score = (2*PR_T*RE_T)/(PR_T+RE_T)\n",
    "        ACC_T = (tp+tn)/(tp+fp+tn+fn)\n",
    "        print(nameDict[model], \":\", tp, tn, fp, fn, ACC_T, F1_Score, PR_T, RE_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments for Kerala\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = svm.SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "knn = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "clf_2 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "\n",
    "\n",
    "nameDict = {dt:\"Tree\", clf:\"SVM\", rf:\"random forest\", gnb:\"NB\", mlp:\"MLP\", mnb:\"MNB\", ada:\"ADA\", bnb:\"BNB\", clf_2:\"penalize\"}\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [dt, clf, rf, gnb, bnb, mlp, mnb, ada, clf_2]\n",
    "# classifiers = [clf_2]\n",
    "\n",
    "for model in classifiers:\n",
    "    kerala = kerala.sample(frac=1)\n",
    "    kf =  KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(kerala):\n",
    "\n",
    "        X_K = kerala.iloc[train_index].drop(columns=[\"label\"])\n",
    "        Y_K = kerala.iloc[train_index][\"label\"]\n",
    "        _X_K = kerala.iloc[test_index].drop(columns=[\"label\"])\n",
    "        _Y_K = kerala.iloc[test_index][\"label\"]\n",
    "        \n",
    "        X_train = X_K\n",
    "        Y_train = Y_K\n",
    "        X_test = _X_K\n",
    "        Y_test = _Y_K\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        tn, fp, fn, tp = mt.confusion_matrix(Y_test, y_pred).ravel()\n",
    "        PR_T = tp/(tp+fp)\n",
    "        RE_T = tp/(tp+fn)\n",
    "        F1_Score = (2*PR_T*RE_T)/(PR_T+RE_T)\n",
    "        ACC_T = (tp+tn)/(tp+fp+tn+fn)\n",
    "        print(nameDict[model], \":\", tp, tn, fp, fn, ACC_T, F1_Score, PR_T, RE_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments for Kerala and Florence => Experiment via  Swapping the datasets\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = svm.SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "knn = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "clf_2 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "\n",
    "\n",
    "nameDict = {dt:\"Tree\", clf:\"SVM\", rf:\"random forest\", gnb:\"NB\", mlp:\"MLP\", mnb:\"MNB\", ada:\"ADA\", bnb:\"BNB\", clf_2:\"penalize\"}\n",
    "\n",
    "X_train = X_Florence\n",
    "Y_train = Y_Florence\n",
    "X_test = X_Kerala\n",
    "Y_test = Y_Kerala\n",
    "\n",
    "classifiers = [clf_2]\n",
    "\n",
    "for model in classifiers:\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    tn, fp, fn, tp = mt.confusion_matrix(Y_test, y_pred).ravel()\n",
    "    PR_T = tp/(tp+fp)\n",
    "    RE_T = tp/(tp+fn)\n",
    "    F1_Score = (2*PR_T*RE_T)/(PR_T+RE_T)\n",
    "    ACC_T = (tp+tn)/(tp+fp+tn+fn)\n",
    "    print(nameDict[model], \":\", tp, tn, fp, fn, ACC_T, F1_Score, PR_T, RE_T)\n",
    "    \n",
    "    prob_y_2 = clf_2.predict_proba(X_train)\n",
    "    prob_y_2 = [p[1] for p in prob_y_2]\n",
    "    print( roc_auc_score(Y_train, prob_y_2) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From now on, we do feature evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Feature evaluation######\n",
    "# First dataset\n",
    "\n",
    "c = DecisionTreeClassifier()\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "extra = ExtraTreesClassifier()\n",
    "\n",
    "ada.fit(X_Kerala, Y_Kerala)\n",
    "featureImportance_ada_k = dict(zip(labels, ada.feature_importances_))\n",
    "\n",
    "c.fit(X_Kerala, Y_Kerala)\n",
    "featureImportance_c_k = dict(zip(labels, c.feature_importances_))\n",
    "\n",
    "extra.fit(X_Kerala, Y_Kerala)\n",
    "featureImportance_extra_k = dict(zip(labels, extra.feature_importances_))\n",
    "\n",
    "featureImportance_k = {p:featureImportance_ada_k[p]+featureImportance_c_k[p]+featureImportance_extra_k[p] for p in featureImportance_ada_k}\n",
    "maxNum = max(featureImportance_k.values())\n",
    "featureImportance_k = {p:featureImportance_k[p]/maxNum for p in featureImportance_k}\n",
    "featureImportance_sorted_k = sorted(featureImportance_k.items(), key=operator.itemgetter(1), reverse=True)\n",
    "featureImportance_pd_k = pd.DataFrame.from_dict(featureImportance_k, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Feature evaluation######\n",
    "# Second dataset\n",
    "\n",
    "c = DecisionTreeClassifier()\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "ada = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "extra = ExtraTreesClassifier()\n",
    "\n",
    "ada.fit(X_Florence, Y_Florence)\n",
    "featureImportance_ada_f = dict(zip(labels, ada.feature_importances_))\n",
    "\n",
    "c.fit(X_Florence, Y_Florence)\n",
    "featureImportance_c_f = dict(zip(labels, c.feature_importances_))\n",
    "\n",
    "extra.fit(X_Florence, Y_Florence)\n",
    "featureImportance_extra_f = dict(zip(labels, extra.feature_importances_))\n",
    "\n",
    "featureImportance_f = {p:featureImportance_ada_f[p]+featureImportance_c_f[p]+featureImportance_extra_f[p] for p in featureImportance_ada_f}\n",
    "maxNum = max(featureImportance_f.values())\n",
    "featureImportance_f = {p:featureImportance_f[p]/maxNum for p in featureImportance_f}\n",
    "featureImportance_sorted_f = sorted(featureImportance_f.items(), key=operator.itemgetter(1), reverse=True)\n",
    "featureImportance_pd_f = pd.DataFrame.from_dict(featureImportance_f, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureImportance_pd_total = featureImportance_pd_f + featureImportance_pd_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = featureImportance_pd_f\n",
    "featureImportance[\"1\"] = featureImportance_pd_k[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = featureImportance.drop(index=['engagementScore', 'credibilityScore', 'tweetPostTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = featureImportance.rename(columns={\"1\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportanceCopy = copy.deepcopy(featureImportance)\n",
    "featureImportanceCopy[\"3\"] = featureImportanceCopy[0]+featureImportanceCopy[1]\n",
    "featureImportanceCopy = featureImportanceCopy.drop(columns=[0,1])\n",
    "featureImportanceCopy = featureImportanceCopy.rename(columns={\"3\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = list(featureImportanceCopy[featureImportanceCopy[0]>0.001].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistents = [i for i in featureImportance.index if (featureImportance.loc[i][0] > 0.001 and featureImportance.loc[i][1] > 0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(consistents) & set(linguisticFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(consistents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'literatureFeatures: {len(set(list(featureImportance_pd_total[featureImportance_pd_total[0]>0.001].index)) & set(literatureFeatures))}')\n",
    "print(f'inspiredFeatures: {len(set(list(featureImportance_pd_total[featureImportance_pd_total[0]>0.001].index)) & set(inspiredFeatures))}')\n",
    "print(f'developedFeatures: {len(set(list(featureImportance_pd_total[featureImportance_pd_total[0]>0.001].index)) & set(developedFeatures))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'consistent literatureFeatures: {len((set(list(featureImportance_pd_f[featureImportance_pd_f[0]>0.001].index)) & set(list(featureImportance_pd_k[featureImportance_pd_k[0]>0.001].index))) & set(literatureFeatures))}')\n",
    "print(f'consistent inspiredFeatures: {len((set(list(featureImportance_pd_f[featureImportance_pd_f[0]>0.001].index)) & set(list(featureImportance_pd_k[featureImportance_pd_k[0]>0.001].index))) & set(inspiredFeatures))}')\n",
    "print(f'consistent developedFeatures: {len((set(list(featureImportance_pd_f[featureImportance_pd_f[0]>0.001].index)) & set(list(featureImportance_pd_k[featureImportance_pd_k[0]>0.001].index))) & set(developedFeatures))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"Counter\",\"#\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"Count\",\"#\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"Organization\",\"Org\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"average\",\"avg\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"first\",\"1st\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"second\",\"2nd\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"third\",\"3rd\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"Coordinating\",\"Coord\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"Subordinating\",\"Sub\"))\n",
    "featureImportance['index'] = featureImportance['index'].apply(lambda x: x.replace(\"Conjunction\",\"Conj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance.index = featureImportance[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = featureImportance.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance.loc['character#'] = [0.86,0.134229]\n",
    "featureImportance.loc['token#'] = [0.019403,0.97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumval=0\n",
    "fig = plt.figure(figsize=(15,20))\n",
    "for col in featureImportance.columns:\n",
    "    plt.barh(featureImportance.index, featureImportance[col], left=cumval, label=col)\n",
    "    cumval = cumval+featureImportance[col]\n",
    "    \n",
    "ax = plt.subplot(111)\n",
    "ax.legend([\"Florence Dataset\", \"Kerala Dataset\"], loc='upper center',\n",
    "          ncol=2, fancybox=True, shadow=True)\n",
    "plt.savefig(\"featureEvaluation.eps\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance[2] = featureImportance[0] + featureImportance[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance_Sorted = featureImportance.sort_values(by=[2], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance_Sorted = featureImportance_Sorted.drop(columns=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumval=0\n",
    "fig = plt.figure(figsize=(15,20))\n",
    "for col in featureImportance_Sorted.columns:\n",
    "    plt.barh(featureImportance_Sorted.index, featureImportance_Sorted[col], left=cumval, label=col)\n",
    "    cumval = cumval+featureImportance_Sorted[col]\n",
    "    \n",
    "ax = plt.subplot(111)\n",
    "ax.legend([\"Florence Dataset\", \"Kerala Dataset\"], loc='upper center',\n",
    "          ncol=2, fancybox=True, shadow=True)\n",
    "plt.savefig(\"featureEvaluation.eps\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The spaces in column names are becasue of order of the columns :D\n",
    "df_featureCategory1 = pd.DataFrame({\" Linguistic & Content\":[61,50,38], \" User\":[17,11,4], \"Meta-Message\":[5,4,3]})\n",
    "df_featureCategory2 = pd.DataFrame({\" Literature\":[34,30,20], \"Proposed\":[49,35,25]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featureCategory1.index = [\"Total\",\"Significant\", \"Consistently significant\"]\n",
    "df_featureCategory2.index = [\"Total\",\"Significant\", \"Consistently significant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featureCategory1.transpose().plot(kind='bar')\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.savefig(\"featureComparison1.eps\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featureCategory2.transpose().plot(kind='bar')\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.savefig(\"featureComparison2.eps\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featureCategory1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumval=0\n",
    "fig = plt.figure()\n",
    "for col in df_featureCategory1.transpose().columns:\n",
    "    plt.bar(df_featureCategory1.transpose().index, df_featureCategory1.transpose()[col], bottom=cumval, label=col)\n",
    "    cumval = cumval+df_featureCategory1.transpose()[col]\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumval=0\n",
    "fig = plt.figure()\n",
    "for col in df_featureCategory2.transpose().columns:\n",
    "    plt.bar(df_featureCategory2.transpose().index, df_featureCategory2.transpose()[col], bottom=cumval, label=col)\n",
    "    cumval = cumval+df_featureCategory2.transpose()[col]\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
